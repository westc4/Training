{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d12e97",
   "metadata": {},
   "source": [
    "\n",
    "# 03 — AudioCraft generator training (MusicGen, unconditional debug)\n",
    "\n",
    "Stage 2: train the MusicGen token LM on the prepared FMA mini dataset, using the compression/codebook model trained in 02. Run this after 01b (dataset prep) and 02 (compression debug). No dataset download or prep happens here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f83c62",
   "metadata": {},
   "source": [
    "## 1) Imports + shared paths/constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eced046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUDIOCRAFT_REPO exists: True\n",
      "AUDIOCRAFT_DORA_DIR: /root/workspace/experiments/audiocraft\n",
      "ffmpeg: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Ensure writable caches for numba/joblib in container environments\n",
    "import os as _os\n",
    "_os.environ.setdefault('NUMBA_CACHE_DIR', '/tmp/numba_cache')\n",
    "_os.environ.setdefault('NUMBA_DISABLE_CACHING', '1')\n",
    "_os.environ.setdefault('JOBLIB_TEMP_FOLDER', '/tmp')\n",
    "\n",
    "import os, sys, subprocess, datetime, json\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "AUDIOCRAFT_REPO = Path(\"/root/workspace/audiocraft\")\n",
    "AUDIOCRAFT_DORA_DIR = Path(\"/root/workspace/experiments/audiocraft\")\n",
    "OUTPUT_DIR = Path(\"/root/workspace/Training/outputs/musicgen_uncond_debug\")\n",
    "\n",
    "DSET = \"audio/fma_small_mini\"\n",
    "CONFIG_PATH = Path(\"/root/workspace/Training/model_config/fma_small_mini.yaml\")\n",
    "EGS_DIR = Path(\"/root/workspace/data/fma_small_mini/egs\")\n",
    "EXPECTED_EGS = [EGS_DIR / \"train\", EGS_DIR / \"valid\"]\n",
    "\n",
    "# Debug-friendly hyperparams; tweak as needed\n",
    "SEGMENT_SECONDS = 10\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = max(2, min(8, (os.cpu_count() or 8) // 4))\n",
    "UPDATES_PER_EPOCH = 30\n",
    "EPOCHS = 1\n",
    "GENERATE_EVERY = 10\n",
    "EVALUATE_EVERY = 10\n",
    "SEED = 1234\n",
    "GENERATE_SAMPLES = 2\n",
    "GENERATE_SECONDS = 8  # duration per sample for inference\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"AUDIOCRAFT_REPO exists: {AUDIOCRAFT_REPO.exists()}\")\n",
    "print(f\"AUDIOCRAFT_DORA_DIR: {AUDIOCRAFT_DORA_DIR}\")\n",
    "try:\n",
    "    ffmpeg_ver = subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, text=True)\n",
    "    first_line = (ffmpeg_ver.stdout or ffmpeg_ver.stderr).splitlines()[0]\n",
    "    print(\"ffmpeg:\", first_line)\n",
    "except FileNotFoundError:\n",
    "    print(\"ffmpeg not found (install via apt if you need extra formats)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2605e48e",
   "metadata": {},
   "source": [
    "## 2) Verify prerequisites exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67fca3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All prerequisite paths are present.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "missing = []\n",
    "for p in [CONFIG_PATH, AUDIOCRAFT_REPO, EGS_DIR]:\n",
    "    if not p.exists():\n",
    "        missing.append(str(p))\n",
    "\n",
    "missing_egs = [str(p) for p in EXPECTED_EGS if not p.exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing required paths: {missing}. Run 01b to prepare data and 02 for codec training.\")\n",
    "if missing_egs:\n",
    "    raise FileNotFoundError(f\"Missing egs folders: {missing_egs}. Re-run 01b_fma_small_mini_downloader.ipynb.\")\n",
    "\n",
    "xps_root = AUDIOCRAFT_DORA_DIR / \"xps\"\n",
    "xps_root.mkdir(parents=True, exist_ok=True)\n",
    "print(\"All prerequisite paths are present.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d6c37",
   "metadata": {},
   "source": [
    "## 3) Find the latest compression checkpoint automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47d1ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using compression XP: 060c08dd | solver: compression\n",
      "Checkpoint: /root/workspace/experiments/audiocraft/xps/060c08dd/checkpoint.th\n",
      "Timestamp: 2026-01-26 12:23:15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using compression XP: 060c08dd | solver: compression\n",
      "Checkpoint: /root/workspace/experiments/audiocraft/xps/060c08dd/checkpoint.th\n",
      "Timestamp: 2026-01-26 12:23:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dora directory: /tmp/audiocraft_root\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using compression XP: 060c08dd | solver: compression\n",
      "Checkpoint: /root/workspace/experiments/audiocraft/xps/060c08dd/checkpoint.th\n",
      "Timestamp: 2026-01-26 12:23:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dora directory: /tmp/audiocraft_root\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression meta: {'sample_rate': 16000, 'channels': 1, 'n_q': 32, 'cardinality': 1024, 'frame_rate': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import yaml\n",
    "from audiocraft.solvers import CompressionSolver\n",
    "\n",
    "def read_solver_from_config(xp_dir: Path):\n",
    "    for candidate in [xp_dir / \"config.yaml\", xp_dir / \".hydra\" / \"config.yaml\", xp_dir / \"hydra-config.yaml\"]:\n",
    "        if candidate.exists():\n",
    "            try:\n",
    "                cfg = yaml.safe_load(candidate.read_text())\n",
    "            except Exception as exc:  # noqa: BLE001\n",
    "                print(f\"[warn] Could not parse {candidate}: {exc}\")\n",
    "                continue\n",
    "            solver = None\n",
    "            if isinstance(cfg, dict):\n",
    "                solver = cfg.get(\"solver\")\n",
    "                if solver is None and isinstance(cfg.get(\"xp\"), dict):\n",
    "                    solver = cfg[\"xp\"].get(\"solver\")\n",
    "            return solver, cfg\n",
    "    return None, None\n",
    "\n",
    "def find_xps(filter_keyword: Optional[str] = None):\n",
    "    xp_root = AUDIOCRAFT_DORA_DIR / \"xps\"\n",
    "    if not xp_root.exists():\n",
    "        return []\n",
    "    results = []\n",
    "    for xp in xp_root.iterdir():\n",
    "        if xp.is_dir():\n",
    "            solver, cfg = read_solver_from_config(xp)\n",
    "            solver_str = str(solver) if solver is not None else \"\"\n",
    "            if filter_keyword and filter_keyword not in solver_str:\n",
    "                continue\n",
    "            results.append((xp, solver_str))\n",
    "    results.sort(key=lambda t: t[0].stat().st_mtime)\n",
    "    return results\n",
    "\n",
    "def pick_checkpoint(xp_dir: Path):\n",
    "    priority = [\n",
    "        \"*best*.pt\", \"*best*.pth\", \"*best*.th\",\n",
    "        \"*latest*.pt\", \"*latest*.pth\", \"*latest*.th\",\n",
    "        \"checkpoint*.pt\", \"checkpoint*.pth\", \"checkpoint*.th\",\n",
    "    ]\n",
    "    def grab(patterns):\n",
    "        for pat in patterns:\n",
    "            files = sorted(xp_dir.rglob(pat), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "            if files:\n",
    "                return files[0]\n",
    "        return None\n",
    "    ckpt = grab(priority)\n",
    "    if ckpt is None:\n",
    "        pool = sorted(list(xp_dir.rglob(\"*.pt\")) + list(xp_dir.rglob(\"*.pth\")) + list(xp_dir.rglob(\"*.th\")),\n",
    "                      key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if pool:\n",
    "            ckpt = pool[0]\n",
    "    return ckpt\n",
    "\n",
    "compression_xps = find_xps(\"compression\")\n",
    "if not compression_xps:\n",
    "    raise FileNotFoundError(\"No compression Dora runs found. Run notebook 02_audiocraft_train_compression_debug.ipynb first.\")\n",
    "\n",
    "compression_dir, compression_solver_name = compression_xps[-1]\n",
    "compression_ckpt = pick_checkpoint(compression_dir)\n",
    "if compression_ckpt is None:\n",
    "    raise FileNotFoundError(f\"No checkpoint file found under {compression_dir}\")\n",
    "\n",
    "ckpt_time = datetime.datetime.fromtimestamp(compression_ckpt.stat().st_mtime)\n",
    "print(f\"Using compression XP: {compression_dir.name} | solver: {compression_solver_name}\")\n",
    "print(f\"Checkpoint: {compression_ckpt}\")\n",
    "print(f\"Timestamp: {ckpt_time:%Y-%m-%d %H:%M:%S}\")\n",
    "\n",
    "compression_model = CompressionSolver.model_from_checkpoint(str(compression_ckpt), device=\"cpu\")\n",
    "COMPRESSION_META = dict(\n",
    "    xp_dir=compression_dir,\n",
    "    ckpt_path=compression_ckpt,\n",
    "    sample_rate=compression_model.sample_rate,\n",
    "    channels=compression_model.channels,\n",
    "    n_q=getattr(compression_model, \"num_codebooks\", None),\n",
    "    cardinality=getattr(compression_model, \"cardinality\", None),\n",
    "    frame_rate=getattr(compression_model, \"frame_rate\", None),\n",
    ")\n",
    "if COMPRESSION_META[\"n_q\"] is None and hasattr(compression_model, \"quantizer\"):\n",
    "    COMPRESSION_META[\"n_q\"] = getattr(compression_model.quantizer, \"n_q\", None)\n",
    "print(\"Compression meta:\", {k: v for k, v in COMPRESSION_META.items() if k not in (\"xp_dir\", \"ckpt_path\")})\n",
    "del compression_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa08316",
   "metadata": {},
   "source": [
    "## 4) Identify generator solver/config (unconditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b46a7952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available musicgen solvers: ['musicgen/debug.yaml', 'musicgen/debug_mini.yaml', 'musicgen/default.yaml', 'musicgen/musicgen_base_32khz.yaml', 'musicgen/musicgen_melody_32khz.yaml', 'musicgen/musicgen_style_32khz.yaml']\n",
      "Available audiogen solvers: ['audiogen/audiogen_base_16khz.yaml', 'audiogen/debug.yaml', 'audiogen/default.yaml']\n",
      "Selected solver: musicgen/default (conditioner=none by default)\n",
      "Overrides for unconditional debug: {'model.lm.model_scale': 'xsmall', 'conditioner': 'none', 'generate.lm.use_sampling': False, 'generate.lm.prompted_samples': False, 'generate.lm.unprompted_samples': True, 'generate.lm.no_text_conditioning': True, 'generate.lm.top_k': 0, 'generate.lm.top_p': 0.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "solver_root = AUDIOCRAFT_REPO / \"config\" / \"solver\"\n",
    "musicgen_solvers = sorted([p.relative_to(solver_root).as_posix() for p in solver_root.glob(\"musicgen/*.yaml\")])\n",
    "audiogen_solvers = sorted([p.relative_to(solver_root).as_posix() for p in solver_root.glob(\"audiogen/*.yaml\")])\n",
    "print(\"Available musicgen solvers:\", musicgen_solvers)\n",
    "print(\"Available audiogen solvers:\", audiogen_solvers)\n",
    "\n",
    "# Choose a minimal, unconditional solver\n",
    "LM_SOLVER = \"musicgen/default\"\n",
    "if not (solver_root / (LM_SOLVER + \".yaml\")).exists():\n",
    "    raise FileNotFoundError(f\"Expected solver config missing: {LM_SOLVER}.yaml\")\n",
    "print(f\"Selected solver: {LM_SOLVER} (conditioner=none by default)\")\n",
    "\n",
    "LM_OVERRIDES = {\n",
    "    \"model.lm.model_scale\": \"xsmall\",\n",
    "    \"conditioner\": \"none\",\n",
    "    \"generate.lm.use_sampling\": False,\n",
    "    \"generate.lm.prompted_samples\": False,\n",
    "    \"generate.lm.unprompted_samples\": True,\n",
    "    \"generate.lm.no_text_conditioning\": True,\n",
    "    \"generate.lm.top_k\": 0,\n",
    "    \"generate.lm.top_p\": 0.0,\n",
    "}\n",
    "print(\"Overrides for unconditional debug:\", LM_OVERRIDES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d4eee4",
   "metadata": {},
   "source": [
    "## 5) Run a small debug training job via Dora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4916a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex, subprocess\n",
    "import tempfile\n",
    "\n",
    "# Record existing generator runs to identify the new one after training\n",
    "pre_gen_dirs = {xp.name for xp, _ in find_xps(\"musicgen\")}\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"AUDIOCRAFT_TEAM\"] = env.get(\"AUDIOCRAFT_TEAM\", \"default\")\n",
    "env[\"AUDIOCRAFT_DORA_DIR\"] = str(AUDIOCRAFT_DORA_DIR)\n",
    "env[\"USER\"] = env.get(\"USER\", \"root\")\n",
    "env[\"PYTHONWARNINGS\"] = \"ignore::FutureWarning,ignore::UserWarning\"\n",
    "env[\"NUMBA_CACHE_DIR\"] = \"/tmp/numba_cache\"\n",
    "env[\"NUMBA_DISABLE_CACHING\"] = \"1\"\n",
    "env[\"JOBLIB_TEMP_FOLDER\"] = \"/tmp\"\n",
    "\n",
    "# Create delay pattern for 32 codebooks\n",
    "delays = list(range(COMPRESSION_META['n_q']))  # [0, 1, 2, ..., 31]\n",
    "\n",
    "# Create a temporary solver config that extends musicgen/default with our specific values\n",
    "temp_solver_config = AUDIOCRAFT_REPO / \"config\" / \"solver\" / \"musicgen\" / \"debug_mini.yaml\"\n",
    "solver_config_content = f\"\"\"# @package __global__\n",
    "\n",
    "defaults:\n",
    "  - musicgen/default\n",
    "  - _self_\n",
    "\n",
    "sample_rate: {COMPRESSION_META['sample_rate']}\n",
    "channels: {COMPRESSION_META['channels']}\n",
    "compression_model_checkpoint: {COMPRESSION_META['ckpt_path']}\n",
    "\n",
    "lm_model: transformer_lm\n",
    "\n",
    "codebooks_pattern:\n",
    "  modeling: delay\n",
    "  delay:\n",
    "    delays: {delays}\n",
    "    flatten_first: 0\n",
    "    empty_initial: 0\n",
    "\n",
    "transformer_lm:\n",
    "  n_q: {COMPRESSION_META['n_q']}\n",
    "  card: {COMPRESSION_META['cardinality']}\n",
    "  dim: 128\n",
    "  num_heads: 4\n",
    "  hidden_scale: 2\n",
    "  num_layers: 3\n",
    "  causal: true\n",
    "  memory_efficient: true\n",
    "  bias_proj: false\n",
    "  bias_ff: false\n",
    "  bias_attn: false\n",
    "  norm_first: true\n",
    "  layer_scale: null\n",
    "  weight_init: gaussian\n",
    "  depthwise_init: current\n",
    "  zero_bias_init: true\n",
    "  attention_as_float32: false\n",
    "\n",
    "dataset:\n",
    "  segment_duration: {SEGMENT_SECONDS}\n",
    "  batch_size: {BATCH_SIZE}\n",
    "  num_workers: {NUM_WORKERS}\n",
    "  min_segment_ratio: 1.0\n",
    "\n",
    "generate:\n",
    "  every: null\n",
    "\n",
    "evaluate:\n",
    "  every: {EVALUATE_EVERY}\n",
    "\n",
    "checkpoint:\n",
    "  save_last: true\n",
    "  save_every: null\n",
    "\n",
    "optim:\n",
    "  epochs: {EPOCHS}\n",
    "  updates_per_epoch: {UPDATES_PER_EPOCH}\n",
    "\n",
    "tokens:\n",
    "  padding_with_special_token: false\n",
    "\n",
    "seed: {SEED}\n",
    "fsdp:\n",
    "  use: false\n",
    "logging:\n",
    "  log_tensorboard: false\n",
    "\"\"\"\n",
    "\n",
    "temp_solver_config.write_text(solver_config_content)\n",
    "print(f\"✓ Created solver config: {temp_solver_config.name}\")\n",
    "print(f\"  • {COMPRESSION_META['n_q']} codebooks × {COMPRESSION_META['cardinality']} cardinality\")\n",
    "print(f\"  • Mini transformer: 3 layers, 128 dim, 4 heads, causal=true\")\n",
    "print(f\"  • {EPOCHS} epoch × {UPDATES_PER_EPOCH} updates, batch {BATCH_SIZE}\")\n",
    "print(f\"  • Checkpoints: save_last=true, generation: disabled during training\")\n",
    "\n",
    "# Now run with the simplified command\n",
    "cmd = [\n",
    "    \"python\",\n",
    "    \"-m\", \"dora\", \"run\",\n",
    "    f\"solver=musicgen/debug_mini\",\n",
    "    f\"dset={DSET}\",\n",
    "    \"conditioner=none\",\n",
    "]\n",
    "print(f\"\\nStarting training...\")\n",
    "\n",
    "# Capture output\n",
    "result = subprocess.run(cmd, cwd=str(AUDIOCRAFT_REPO), env=env, check=False, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"\\n❌ Training failed\")\n",
    "    print(\"\\n=== STDERR (last 8000 chars) ===\")\n",
    "    print(result.stderr[-8000:] if len(result.stderr) > 8000 else result.stderr)\n",
    "    if temp_solver_config.exists():\n",
    "        temp_solver_config.unlink()\n",
    "    raise subprocess.CalledProcessError(result.returncode, cmd, result.stdout, result.stderr)\n",
    "else:\n",
    "    # Show training progress\n",
    "    print(\"\\n✓ Training completed!\")\n",
    "    stderr_lines = result.stderr.splitlines()\n",
    "    if stderr_lines:\n",
    "        progress_lines = [l for l in stderr_lines if any(x in l for x in ['Train', 'Valid', 'Evaluate', 'Model size', 'checkpoint', 'Saving'])]\n",
    "        if progress_lines:\n",
    "            print(\"\\nKey training logs:\")\n",
    "            for line in progress_lines[-25:]:\n",
    "                # Strip ANSI codes for cleaner output\n",
    "                clean_line = line.replace('[36m', '').replace('[34m', '').replace('[32m', '').replace('[0m', '')\n",
    "                print(clean_line)\n",
    "\n",
    "# Clean up temp config\n",
    "if temp_solver_config.exists():\n",
    "    temp_solver_config.unlink()\n",
    "\n",
    "post_gen = find_xps(\"musicgen\")\n",
    "new_gen = [xp for xp, _ in post_gen if xp.name not in pre_gen_dirs]\n",
    "if not post_gen:\n",
    "    raise FileNotFoundError(\"No MusicGen runs found after training.\")\n",
    "GEN_XP_DIR = new_gen[-1] if new_gen else post_gen[-1][0]\n",
    "print(f\"\\n✓ Generator XP created: {GEN_XP_DIR.name}\")\n",
    "print(f\"  Location: {GEN_XP_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3700ff",
   "metadata": {},
   "source": [
    "## 5b) Train a MusicGen model (FULL RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65635ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "/root/workspace/audiocraft/config/dset/audio/fma_small_mini.yaml\n",
      "/root/workspace/data/fma_small_mini/egs/train/data.jsonl\n",
      "/root/workspace/data/fma_small_mini/egs/valid/data.jsonl\n",
      "/root/workspace/audiocraft\n",
      "Using compression checkpoint: /root/workspace/experiments/audiocraft/xps/060c08dd/checkpoint.th\n",
      "Compression model: 16000Hz, 32 codebooks, card=1024\n",
      "\n",
      "Using config: dset=audio/fma_small_mini, solver=musicgen/musicgen_base_32khz\n",
      "Training params: segment_duration=10, batch_size=64, num_workers=16\n",
      "Optimizer: updates_per_epoch=50\n",
      "Validation: num_samples=30\n",
      "Evaluate every: 10\n",
      "Autocast: True\n",
      "Generate every: 10\n",
      "Num threads: 16\n",
      "MP start method: fork\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dora directory: /root/workspace/experiments/audiocraft\n",
      "[\u001b[36m01-26 13:37:41\u001b[0m][\u001b[34mdora.distrib\u001b[0m][\u001b[32mINFO\u001b[0m] - world_size is 1, skipping init.\u001b[0m\n",
      "[\u001b[36m01-26 13:37:41\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Instantiating solver MusicGenSolver for XP bd119d3b\u001b[0m\n",
      "[\u001b[36m01-26 13:37:41\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - All XP logs are stored in /root/workspace/experiments/audiocraft/xps/bd119d3b\u001b[0m\n",
      "[\u001b[36m01-26 13:37:41\u001b[0m][\u001b[34maudiocraft.solvers.builders\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading audio data split train: /root/workspace/data/fma_small_mini/egs/train\u001b[0m\n",
      "[\u001b[36m01-26 13:37:41\u001b[0m][\u001b[34maudiocraft.solvers.builders\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading audio data split valid: /root/workspace/data/fma_small_mini/egs/valid\u001b[0m\n",
      "[\u001b[36m01-26 13:37:41\u001b[0m][\u001b[34maudiocraft.solvers.builders\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading audio data split evaluate: /root/workspace/data/fma_small_mini/egs/valid\u001b[0m\n",
      "[\u001b[36m01-26 13:37:41\u001b[0m][\u001b[34maudiocraft.solvers.builders\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading audio data split generate: /root/workspace/data/fma_small_mini/egs/valid\u001b[0m\n",
      "[\u001b[36m01-26 13:37:41\u001b[0m][\u001b[34maudiocraft.solvers.compression\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading compression model from checkpoint: /root/workspace/experiments/audiocraft/xps/060c08dd/checkpoint.th\u001b[0m\n",
      "[\u001b[36m01-26 13:37:41\u001b[0m][\u001b[34maudiocraft.utils.checkpoint\u001b[0m][\u001b[32mINFO\u001b[0m] - Checkpoint loaded from /root/workspace/experiments/audiocraft/xps/060c08dd/checkpoint.th\u001b[0m\n",
      "[\u001b[36m01-26 13:37:41\u001b[0m][\u001b[34maudiocraft.solvers.compression\u001b[0m][\u001b[32mINFO\u001b[0m] - Compression model loaded!\u001b[0m\n",
      "[\u001b[36m01-26 13:37:41\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Compression model has 32 codebooks with 1024 cardinality, and a framerate of 50\u001b[0m\n",
      "[\u001b[36m01-26 13:37:42\u001b[0m][\u001b[34maudiocraft.optim.dadam\u001b[0m][\u001b[32mINFO\u001b[0m] - Using decoupled weight decay\u001b[0m\n",
      "[\u001b[36m01-26 13:37:45\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Model hash: 7c388638df66b5bc2496b5ee07d4d33b84617710\u001b[0m\n",
      "[\u001b[36m01-26 13:37:45\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Initializing EMA on the model with decay = 0.99 every 10 updates\u001b[0m\n",
      "[\u001b[36m01-26 13:37:45\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Model size: 369.23 M params\u001b[0m\n",
      "[\u001b[36m01-26 13:37:45\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Base memory usage, with model, grad and optim: 5.91 GB\u001b[0m\n",
      "[\u001b[36m01-26 13:37:45\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Restoring weights and history.\u001b[0m\n",
      "[\u001b[36m01-26 13:37:47\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Model hash: 7c388638df66b5bc2496b5ee07d4d33b84617710\u001b[0m\n",
      "[\u001b[36m01-26 13:37:51\u001b[0m][\u001b[34maudiocraft.modules.codebooks_patterns\u001b[0m][\u001b[32mINFO\u001b[0m] - New pattern, time steps: 500, sequence steps: 532\u001b[0m\n",
      "[\u001b[36m01-26 13:37:59\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 5/50 | 0.54 it/sec | lr 6.25E-04 | grad_norm 8.873E+00 | grad_scale 65536.000 | ce 7.399 | ppl 1635.046\u001b[0m\n",
      "[\u001b[36m01-26 13:38:04\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 10/50 | 0.66 it/sec | lr 2.00E-03 | grad_norm 8.816E+00 | grad_scale 65536.000 | ce 7.398 | ppl 1632.209\u001b[0m\n",
      "[\u001b[36m01-26 13:38:09\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 15/50 | 0.73 it/sec | lr 3.25E-03 | grad_norm 8.785E+00 | grad_scale 65536.000 | ce 7.396 | ppl 1629.455\u001b[0m\n",
      "[\u001b[36m01-26 13:38:15\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 20/50 | 0.75 it/sec | lr 4.50E-03 | grad_norm 8.711E+00 | grad_scale 65536.000 | ce 7.395 | ppl 1627.852\u001b[0m\n",
      "[\u001b[36m01-26 13:38:21\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 25/50 | 0.78 it/sec | lr 5.75E-03 | grad_norm 8.749E+00 | grad_scale 65536.000 | ce 7.394 | ppl 1625.664\u001b[0m\n",
      "[\u001b[36m01-26 13:38:26\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 30/50 | 0.79 it/sec | lr 7.00E-03 | grad_norm 8.704E+00 | grad_scale 65536.000 | ce 7.391 | ppl 1621.363\u001b[0m\n",
      "[\u001b[36m01-26 13:38:32\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 35/50 | 0.80 it/sec | lr 8.25E-03 | grad_norm 8.790E+00 | grad_scale 65536.000 | ce 7.391 | ppl 1621.587\u001b[0m\n",
      "[\u001b[36m01-26 13:38:38\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 40/50 | 0.81 it/sec | lr 9.50E-03 | grad_norm 8.826E+00 | grad_scale 65536.000 | ce 7.390 | ppl 1620.136\u001b[0m\n",
      "[\u001b[36m01-26 13:38:43\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 45/50 | 0.82 it/sec | lr 1.07E-02 | grad_norm 8.780E+00 | grad_scale 65536.000 | ce 7.386 | ppl 1612.937\u001b[0m\n",
      "[\u001b[36m01-26 13:38:51\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[1mTrain Summary | Epoch 1 | lr=6.13E-03 | grad_norm=8.784E+00 | grad_scale=65536.000 | ce=7.393 | ppl=1624.045 | duration=64.227\u001b[0m\n",
      "[\u001b[36m01-26 13:38:55\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[1mValid Summary | Epoch 1 | ce=7.389 | ppl=1618.137 | duration=3.236\u001b[0m\n",
      "[\u001b[36m01-26 13:38:55\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - New best state with ce=7.389 (was inf)\u001b[0m\n",
      "[\u001b[36m01-26 13:38:58\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Model hash: c4c3693a878eb0607ce41c712d712eedb9b95575\u001b[0m\n",
      "[\u001b[36m01-26 13:39:44\u001b[0m][\u001b[34maudiocraft.utils.checkpoint\u001b[0m][\u001b[32mINFO\u001b[0m] - Checkpoint saved to /root/workspace/experiments/audiocraft/xps/bd119d3b/checkpoint.th\u001b[0m\n",
      "[\u001b[36m01-26 13:39:56\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 2 | 5/50 | 0.62 it/sec | lr 1.31E-02 | grad_norm 8.721E+00 | grad_scale 65536.000 | ce 7.379 | ppl 1602.373\u001b[0m\n",
      "[\u001b[36m01-26 13:40:02\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 2 | 10/50 | 0.73 it/sec | lr 1.45E-02 | grad_norm 8.833E+00 | grad_scale 65536.000 | ce 7.378 | ppl 1600.882\u001b[0m\n",
      "[\u001b[36m01-26 13:40:07\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 2 | 15/50 | 0.78 it/sec | lr 1.58E-02 | grad_norm 8.785E+00 | grad_scale 65536.000 | ce 7.373 | ppl 1592.318\u001b[0m\n",
      "[\u001b[36m01-26 13:40:13\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 2 | 20/50 | 0.79 it/sec | lr 1.70E-02 | grad_norm 8.789E+00 | grad_scale 65536.000 | ce 7.369 | ppl 1586.465\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 90\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMP start method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMP_START_METHOD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m cmd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124mcd \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAUDIOCRAFT_REPO_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m && \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124mpython -m dora run \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124m  mp_start_method=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMP_START_METHOD\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/lib/python3.11/subprocess.py:1201\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/subprocess.py:1264\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/subprocess.py:2053\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 2053\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/usr/lib/python3.11/subprocess.py:2011\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2011\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mwaitpid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid, wait_flags)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   2013\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   2015\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   2016\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/dora/__main__.py\", line 174, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/dora/__main__.py\", line 170, in main\n",
      "    args.action(args, main)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/dora/run.py\", line 69, in run_action\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/dora/main.py\", line 86, in __call__\n",
      "    return self._main()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/dora/hydra.py\", line 228, in _main\n",
      "    return hydra.main(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/hydra/main.py\", line 94, in decorated_main\n",
      "    _run_hydra(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n",
      "    _run_app(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/hydra/_internal/utils.py\", line 457, in _run_app\n",
      "    run_and_report(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/hydra/_internal/utils.py\", line 220, in run_and_report\n",
      "    return func()\n",
      "           ^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/hydra/_internal/utils.py\", line 458, in <lambda>\n",
      "    lambda: hydra.run(\n",
      "            ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/hydra/_internal/hydra.py\", line 119, in run\n",
      "    ret = run_job(\n",
      "          ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/hydra/core/utils.py\", line 186, in run_job\n",
      "    ret.return_value = task_function(task_cfg)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/workspace/audiocraft/audiocraft/train.py\", line 152, in main\n",
      "    return solver.run()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/root/workspace/audiocraft/audiocraft/solvers/base.py\", line 497, in run\n",
      "    self.run_epoch()\n",
      "  File \"/root/workspace/audiocraft/audiocraft/solvers/musicgen.py\", line 621, in run_epoch\n",
      "    super().run_epoch()\n",
      "  File \"/root/workspace/audiocraft/audiocraft/solvers/base.py\", line 477, in run_epoch\n",
      "    self.run_stage('train', self.train)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/flashy/solver.py\", line 199, in run_stage\n",
      "    metrics = method(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/workspace/audiocraft/audiocraft/solvers/musicgen.py\", line 634, in train\n",
      "    return super().train()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/root/workspace/audiocraft/audiocraft/solvers/base.py\", line 561, in train\n",
      "    return self.common_train_valid('train')\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/workspace/audiocraft/audiocraft/solvers/base.py\", line 542, in common_train_valid\n",
      "    metrics = self.run_step(idx, batch, metrics)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/workspace/audiocraft/audiocraft/solvers/musicgen.py\", line 404, in run_step\n",
      "    loss.backward()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Single switch for all paths (defaults to your new location)\n",
    "BASE_DIR = Path(os.environ.get(\"WORKSPACE_DIR\", \"/root/workspace\"))\n",
    "\n",
    "AUDIOCRAFT_REPO_DIR = BASE_DIR / \"audiocraft\"\n",
    "EXPERIMENTS_DIR     = BASE_DIR / \"experiments\" / \"audiocraft\"\n",
    "\n",
    "DSET = \"audio/fma_small_mini\"\n",
    "SOLVER = \"musicgen/musicgen_base_32khz\"  # MusicGen solver, not compression\n",
    "\n",
    "SEGMENT_SECONDS = 10\n",
    "BATCH_SIZE = 64  # Increased from 8 to improve GPU utilization\n",
    "\n",
    "# Auto-pick workers: reduce to 4-6 to avoid CPU contention\n",
    "_cpu_count = os.cpu_count() or 32\n",
    "# NUM_WORKERS = min(16, max(8, _cpu_count // 4))   # 8–16 is usually the sweet spot\n",
    "NUM_WORKERS = 16 # this worked well on v84 cpu\n",
    "\n",
    "UPDATES_PER_EPOCH = 50\n",
    "VALID_NUM_SAMPLES = 30\n",
    "GENERATE_EVERY = 10\n",
    "EVALUATE_EVERY = 10\n",
    "AUTOCAST = True\n",
    "NUM_THREADS = 16  # Set number of threads for PyTorch operations\n",
    "MP_START_METHOD = \"fork\" # Use 'fork' to reduce overhead on Linux systems\n",
    "# MP_START_METHOD = \"forkserver\" # Alternative method if issues arise with 'fork'\n",
    "\n",
    "CONFIG_PATH = AUDIOCRAFT_REPO_DIR / \"config\" / \"dset\" / \"audio\" / \"fma_small_mini.yaml\"\n",
    "TRAIN_JSONL  = BASE_DIR / \"data\" / \"fma_small_mini\" / \"egs\" / \"train\" / \"data.jsonl\"\n",
    "VALID_JSONL  = BASE_DIR / \"data\" / \"fma_small_mini\" / \"egs\" / \"valid\" / \"data.jsonl\"\n",
    "\n",
    "# Use the compression checkpoint from the earlier training\n",
    "COMPRESSION_CHECKPOINT = str(COMPRESSION_META['ckpt_path'])\n",
    "\n",
    "# Create delay pattern for all codebooks\n",
    "delays_str = \"[\" + \",\".join(str(i) for i in range(COMPRESSION_META['n_q'])) + \"]\"\n",
    "\n",
    "print(NUM_WORKERS)\n",
    "print(CONFIG_PATH)\n",
    "print(TRAIN_JSONL)\n",
    "print(VALID_JSONL)\n",
    "print(AUDIOCRAFT_REPO_DIR)\n",
    "print(f\"Using compression checkpoint: {COMPRESSION_CHECKPOINT}\")\n",
    "print(f\"Compression model: {COMPRESSION_META['sample_rate']}Hz, {COMPRESSION_META['n_q']} codebooks, card={COMPRESSION_META['cardinality']}\")\n",
    "\n",
    "# Setup environment\n",
    "env = os.environ.copy()\n",
    "env['AUDIOCRAFT_TEAM'] = 'default'\n",
    "env['AUDIOCRAFT_DORA_DIR'] = str(EXPERIMENTS_DIR)\n",
    "env['USER'] = env.get('USER', 'root')\n",
    "env['PYTHONWARNINGS'] = 'ignore::FutureWarning,ignore::UserWarning'\n",
    "\n",
    "print(f\"\\nUsing config: dset={DSET}, solver={SOLVER}\")\n",
    "print(f\"Training params: segment_duration={SEGMENT_SECONDS}, batch_size={BATCH_SIZE}, num_workers={NUM_WORKERS}\")\n",
    "print(f\"Optimizer: updates_per_epoch={UPDATES_PER_EPOCH}\")\n",
    "print(f\"Validation: num_samples={VALID_NUM_SAMPLES}\")\n",
    "print(f\"Evaluate every: {EVALUATE_EVERY}\")\n",
    "print(f\"Autocast: {AUTOCAST}\")\n",
    "print(f\"Generate every: {GENERATE_EVERY}\")\n",
    "print(f\"Num threads: {NUM_THREADS}\")\n",
    "print(f\"MP start method: {MP_START_METHOD}\")\n",
    "\n",
    "cmd = f\"\"\"\n",
    "cd {AUDIOCRAFT_REPO_DIR} && \\\n",
    "python -m dora run \\\n",
    "  solver={SOLVER} \\\n",
    "  dset={DSET} \\\n",
    "  compression_model_checkpoint={COMPRESSION_CHECKPOINT} \\\n",
    "  sample_rate={COMPRESSION_META['sample_rate']} \\\n",
    "  channels={COMPRESSION_META['channels']} \\\n",
    "  transformer_lm.card={COMPRESSION_META['cardinality']} \\\n",
    "  transformer_lm.n_q={COMPRESSION_META['n_q']} \\\n",
    "  'codebooks_pattern.delay.delays={delays_str}' \\\n",
    "  conditioner=none \\\n",
    "  dataset.segment_duration={SEGMENT_SECONDS} \\\n",
    "  dataset.batch_size={BATCH_SIZE} \\\n",
    "  dataset.num_workers={NUM_WORKERS} \\\n",
    "  optim.updates_per_epoch={UPDATES_PER_EPOCH} \\\n",
    "  dataset.valid.num_samples={VALID_NUM_SAMPLES} \\\n",
    "  generate.every={GENERATE_EVERY} \\\n",
    "  evaluate.every={EVALUATE_EVERY} \\\n",
    "  autocast={str(AUTOCAST).lower()} \\\n",
    "  num_threads={NUM_THREADS} \\\n",
    "  mp_start_method={MP_START_METHOD}\n",
    "\"\"\"\n",
    "\n",
    "subprocess.run(cmd, shell=True, check=True, env=env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f41b2b",
   "metadata": {},
   "source": [
    "## 6) Locate generator checkpoint and generate audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5ff61c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator checkpoint info:\n",
      "  XP: bd119d3b\n",
      "  Checkpoint: checkpoint.th\n",
      "  Compression: checkpoint.th\n",
      "\n",
      "Loading model on cuda...\n",
      "Loading compression model from /root/workspace/experiments/audiocraft/xps/060c08dd/checkpoint.th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LM model...\n",
      "Loading LM weights from checkpoint.th...\n",
      "✓ Model loaded: 400 tokens (8s at 50 Hz)\n",
      "\n",
      "Generating 2 samples...\n",
      "Saving audio files to /root/workspace/Training/outputs/musicgen_uncond_debug...\n",
      "\n",
      "✓ Generated samples:\n",
      "  generated_000.wav: 8.0s, RMS=0.0142, peak=0.1826\n",
      "  generated_001.wav: 8.0s, RMS=0.0142, peak=0.1826\n",
      "\n",
      "Summary:\n",
      "{\n",
      "  \"compression_checkpoint\": \"/root/workspace/experiments/audiocraft/xps/060c08dd/checkpoint.th\",\n",
      "  \"generator_checkpoint\": \"/root/workspace/experiments/audiocraft/xps/bd119d3b/checkpoint.th\",\n",
      "  \"sample_rate\": 16000,\n",
      "  \"num_codebooks\": 32,\n",
      "  \"cardinality\": 1024,\n",
      "  \"generated\": [\n",
      "    {\n",
      "      \"path\": \"/root/workspace/Training/outputs/musicgen_uncond_debug/generated_000.wav\",\n",
      "      \"duration_sec\": 8.0,\n",
      "      \"rms\": 0.0142,\n",
      "      \"peak\": 0.1826\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"/root/workspace/Training/outputs/musicgen_uncond_debug/generated_001.wav\",\n",
      "      \"duration_sec\": 8.0,\n",
      "      \"rms\": 0.0142,\n",
      "      \"peak\": 0.1826\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from audiocraft.solvers import CompressionSolver\n",
    "from audiocraft.utils import checkpoint\n",
    "from audiocraft.models.builders import get_lm_model\n",
    "import torch, torchaudio\n",
    "import omegaconf\n",
    "\n",
    "# Re-discover in case the session was reloaded\n",
    "musicgen_xps = find_xps(\"musicgen\")\n",
    "if not musicgen_xps:\n",
    "    musicgen_xps = find_xps(\"audiogen\")\n",
    "if not musicgen_xps:\n",
    "    raise FileNotFoundError(\"No generator XP found. Run the training cell above.\")\n",
    "\n",
    "GEN_XP_DIR = musicgen_xps[-1][0]\n",
    "GEN_CKPT = pick_checkpoint(GEN_XP_DIR)\n",
    "if GEN_CKPT is None:\n",
    "    raise FileNotFoundError(f\"No checkpoint found under {GEN_XP_DIR}\")\n",
    "\n",
    "print(\"Generator checkpoint info:\")\n",
    "print(f\"  XP: {GEN_XP_DIR.name}\")\n",
    "print(f\"  Checkpoint: {GEN_CKPT.name}\")\n",
    "print(f\"  Compression: {COMPRESSION_META['ckpt_path'].name}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\nLoading model on {device}...\")\n",
    "\n",
    "try:\n",
    "    # Load the config\n",
    "    hydra_config = GEN_XP_DIR / \".hydra\" / \"config.yaml\"\n",
    "    if not hydra_config.exists():\n",
    "        raise FileNotFoundError(f\"Config not found at {hydra_config}\")\n",
    "    \n",
    "    cfg = omegaconf.OmegaConf.load(hydra_config)\n",
    "    cfg.device = device\n",
    "    \n",
    "    # Load compression model\n",
    "    print(f\"Loading compression model from {COMPRESSION_META['ckpt_path']}...\")\n",
    "    compression_model = CompressionSolver.model_from_checkpoint(COMPRESSION_META['ckpt_path'], device=device)\n",
    "    frame_rate = compression_model.frame_rate\n",
    "    \n",
    "    # Load LM model\n",
    "    print(f\"Building LM model...\")\n",
    "    lm_model = get_lm_model(cfg)\n",
    "    \n",
    "    # Load model state from checkpoint\n",
    "    print(f\"Loading LM weights from {GEN_CKPT.name}...\")\n",
    "    checkpoint_data = checkpoint.load_checkpoint(GEN_CKPT, is_sharded=False)\n",
    "    lm_model.load_state_dict(checkpoint_data['model'])\n",
    "    lm_model.to(device)\n",
    "    lm_model.eval()\n",
    "    \n",
    "    max_gen_len = int(GENERATE_SECONDS * frame_rate)\n",
    "    \n",
    "    print(f\"✓ Model loaded: {max_gen_len} tokens ({GENERATE_SECONDS}s at {frame_rate} Hz)\")\n",
    "    \n",
    "    print(f\"\\nGenerating {GENERATE_SAMPLES} samples...\")\n",
    "    with torch.no_grad():\n",
    "        tokens = lm_model.generate(\n",
    "            prompt=None,\n",
    "            num_samples=GENERATE_SAMPLES,\n",
    "            max_gen_len=max_gen_len,\n",
    "            use_sampling=False,\n",
    "            top_k=0,\n",
    "            top_p=0.0,\n",
    "        )\n",
    "        audio = compression_model.decode(tokens)\n",
    "    \n",
    "    audio = audio.detach().cpu()\n",
    "    sample_rate = int(compression_model.sample_rate)\n",
    "    saved = []\n",
    "    \n",
    "    print(f\"Saving audio files to {OUTPUT_DIR}...\")\n",
    "    for i in range(audio.shape[0]):\n",
    "        wav = audio[i]\n",
    "        if wav.dim() == 1:\n",
    "            wav = wav.unsqueeze(0)\n",
    "        out_path = OUTPUT_DIR / f\"generated_{i:03d}.wav\"\n",
    "        torchaudio.save(str(out_path), wav, sample_rate)\n",
    "        duration = wav.shape[-1] / sample_rate\n",
    "        rms = torch.sqrt(torch.mean(wav ** 2)).item()\n",
    "        peak = torch.max(torch.abs(wav)).item()\n",
    "        saved.append({\"path\": str(out_path), \"duration_sec\": round(duration, 2), \"rms\": round(rms, 4), \"peak\": round(peak, 4)})\n",
    "    \n",
    "    print(\"\\n✓ Generated samples:\")\n",
    "    for item in saved:\n",
    "        print(f\"  {Path(item['path']).name}: {item['duration_sec']}s, RMS={item['rms']}, peak={item['peak']}\")\n",
    "    \n",
    "    SUMMARY = {\n",
    "        \"compression_checkpoint\": str(COMPRESSION_META[\"ckpt_path\"]),\n",
    "        \"generator_checkpoint\": str(GEN_CKPT),\n",
    "        \"sample_rate\": sample_rate,\n",
    "        \"num_codebooks\": COMPRESSION_META[\"n_q\"],\n",
    "        \"cardinality\": COMPRESSION_META[\"cardinality\"],\n",
    "        \"generated\": saved,\n",
    "    }\n",
    "    print(\"\\nSummary:\")\n",
    "    print(json.dumps(SUMMARY, indent=2))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Generation failed: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6582f81",
   "metadata": {},
   "source": [
    "## 7) Minimal sanity evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeace70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torchaudio\n",
    "\n",
    "check_paths = [Path(item[\"path\"]) for item in saved]\n",
    "for p in check_paths:\n",
    "    wav, sr = torchaudio.load(p)\n",
    "    duration = wav.shape[-1] / sr\n",
    "    finite = torch.isfinite(wav).all().item()\n",
    "    print(f\"{p.name}: sr={sr}, duration={duration:.2f}s, finite={finite}, rms={wav.pow(2).mean().sqrt().item():.4f}, peak={wav.abs().max().item():.4f}\")\n",
    "    if sr != sample_rate:\n",
    "        raise ValueError(f\"Unexpected sample rate in {p}: {sr} (expected {sample_rate})\")\n",
    "    if not finite:\n",
    "        raise ValueError(f\"NaNs detected in {p}\")\n",
    "print(\"Sanity checks complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
