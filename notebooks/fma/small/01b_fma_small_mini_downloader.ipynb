{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01b — FMA large mini downloader\n",
        "Sections 6–13 (download + preprocessing) split out from `01_all_data_setup.ipynb`.\n",
        "Run the setup notebook first so system/Python deps and the AudioCraft repo are available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20898701",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "BASE_DIR = Path(\"/root/workspace\")\n",
        "DATA_DIR = BASE_DIR / \"data\" / \"all_data\"\n",
        "RAW_DIR = BASE_DIR / \"data\" / \"fma_raw\"\n",
        "AUDIOCRAFT_REPO_DIR = BASE_DIR / \"audiocraft\"\n",
        "EXPERIMENTS_DIR = BASE_DIR / \"experiments\" / \"audiocraft\"\n",
        "\n",
        "SEGMENT_SECONDS = 60\n",
        "TARGET_SR = 32000\n",
        "CHANNELS = 1\n",
        "TRAIN_RATIO = 0.9\n",
        "RANDOM_SEED = 42\n",
        "NUM_SAMPLES_TOTAL = None  # adjust to control how many source tracks to keep\n",
        "\n",
        "FMA_ARCHIVE_URLS = [\n",
        "    os.environ.get(\"FMA_SAMPLE_ARCHIVE_URL\"),\n",
        "    \"https://os.unil.cloud.switch.ch/fma/all_data.zip\",\n",
        "    \"https://mirror.math.princeton.edu/pub/fma/all_data.zip\",\n",
        "    \"https://huggingface.co/datasets/echonest/all_data/resolve/main/all_data.zip\",\n",
        "]\n",
        "FMA_ARCHIVE_URLS = [u for u in FMA_ARCHIVE_URLS if u]\n",
        "\n",
        "WAV_DIR = DATA_DIR / \"wav_32k_mono\"\n",
        "SEGMENTS_DIR = DATA_DIR / \"segments_60s\"\n",
        "MANIFEST_DIR = DATA_DIR / \"manifests\"\n",
        "EGS_TRAIN = DATA_DIR / \"egs\" / \"train\"\n",
        "EGS_VALID = DATA_DIR / \"egs\" / \"valid\"\n",
        "\n",
        "for p in (DATA_DIR, RAW_DIR, WAV_DIR, SEGMENTS_DIR, MANIFEST_DIR, EGS_TRAIN, EGS_VALID, EXPERIMENTS_DIR):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"BASE_DIR:\", BASE_DIR)\n",
        "print(\"Using URLs (in order):\", FMA_ARCHIVE_URLS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03fd04b6",
      "metadata": {},
      "source": [
        "## 6) Download a large FMA subset\n",
        "Downloads `all_data.zip` (or a user-provided largeer archive) and extracts a limited number of MP3s.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7910d99",
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import os\n",
        "\n",
        "archive_path = RAW_DIR / \"all_data.zip\"\n",
        "mp3_root = RAW_DIR / \"all_data\"\n",
        "\n",
        "def aria2_download(url: str, out_path: Path):\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    cmd = [\n",
        "        \"aria2c\",\n",
        "        \"-x\", \"16\",          # connections per server\n",
        "        \"-s\", \"16\",          # split count\n",
        "        \"-k\", \"1M\",          # chunk size\n",
        "        \"--file-allocation=none\",\n",
        "        \"--allow-overwrite=true\",\n",
        "        \"-o\", out_path.name,\n",
        "        \"-d\", str(out_path.parent),\n",
        "        url,\n",
        "    ]\n",
        "    print(\"Running:\", \" \".join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "def extract_member(args):\n",
        "    \"\"\"Extract a single member from the zip archive\"\"\"\n",
        "    archive_path, member, raw_dir = args\n",
        "    dest = raw_dir / member\n",
        "    if dest.exists():\n",
        "        return False\n",
        "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with zipfile.ZipFile(archive_path) as zf:\n",
        "        with zf.open(member) as src, open(dest, \"wb\") as dst:\n",
        "            dst.write(src.read())\n",
        "    return True\n",
        "\n",
        "if not any(mp3_root.rglob(\"*.mp3\")):\n",
        "    if not archive_path.exists():\n",
        "        if not FMA_ARCHIVE_URLS:\n",
        "            raise RuntimeError(\"Provide at least one FMA archive URL (set FMA_SAMPLE_ARCHIVE_URL)\")\n",
        "        last_err = None\n",
        "        for url in FMA_ARCHIVE_URLS:\n",
        "            try:\n",
        "                print(f\"Downloading {url} → {archive_path} (aria2c multi-conn)\")\n",
        "                aria2_download(url, archive_path)\n",
        "                break\n",
        "            except Exception as e:  # noqa: BLE001\n",
        "                last_err = e\n",
        "                print(f\"Failed {url}: {e}\")\n",
        "        if not archive_path.exists():\n",
        "            raise RuntimeError(f\"Download failed; last error: {last_err}\")\n",
        "\n",
        "    with zipfile.ZipFile(archive_path) as zf:\n",
        "        members = sorted([m for m in zf.namelist() if m.endswith(\".mp3\")])\n",
        "        if NUM_SAMPLES_TOTAL:\n",
        "            members = members[:NUM_SAMPLES_TOTAL]\n",
        "        print(f\"Extracting {len(members)} MP3s from archive (parallel)…\")\n",
        "        \n",
        "        # Prepare arguments for parallel extraction\n",
        "        extract_args = [(archive_path, member, RAW_DIR) for member in members]\n",
        "        \n",
        "        # Use ThreadPoolExecutor for parallel extraction\n",
        "        max_workers = min(os.cpu_count() or 4, 48)  # Use up to 8 workers\n",
        "        # max_workers = 100\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            results = list(tqdm(\n",
        "                executor.map(extract_member, extract_args),\n",
        "                total=len(extract_args),\n",
        "                desc=\"Extracting\",\n",
        "                unit=\" files\"\n",
        "            ))\n",
        "        \n",
        "        extracted_count = sum(results)\n",
        "        print(f\"Extracted {extracted_count} new files, {len(results) - extracted_count} already existed\")\n",
        "else:\n",
        "    print(\"MP3s already present; skipping download/extract.\")\n",
        "\n",
        "sampled_mp3s = sorted(mp3_root.rglob(\"*.mp3\"))\n",
        "print(\"MP3 files ready:\", len(sampled_mp3s))\n",
        "for name in sampled_mp3s[:5]:\n",
        "    print(\" •\", name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04bcb30f",
      "metadata": {},
      "source": [
        "## 7) Convert to mono 32k wav\n",
        "Uses ffmpeg; idempotent if WAVs already exist.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fc789f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "WAV_DIR.mkdir(parents=True, exist_ok=True)\n",
        "mp3_files = sorted((RAW_DIR / \"all_data\").rglob(\"*.mp3\"))\n",
        "\n",
        "def convert_to_wav(src):\n",
        "    \"\"\"Convert a single MP3 file to WAV\"\"\"\n",
        "    dst = WAV_DIR / f\"{src.stem}.wav\"\n",
        "    if dst.exists():\n",
        "        return \"skip\"\n",
        "    try:\n",
        "        cmd = [\n",
        "            \"ffmpeg\", \"-hide_banner\", \"-loglevel\", \"error\",\n",
        "            \"-i\", str(src),\n",
        "            \"-ac\", str(CHANNELS),\n",
        "            \"-ar\", str(TARGET_SR),\n",
        "            \"-map_metadata\", \"-1\",\n",
        "            \"-vn\",\n",
        "            str(dst),\n",
        "        ]\n",
        "        subprocess.run(cmd, check=True)\n",
        "        return \"success\"\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # If conversion fails, log and skip the corrupted file\n",
        "        return (\"error\", src)\n",
        "\n",
        "# Use ThreadPoolExecutor for parallel conversion\n",
        "max_workers = min(os.cpu_count() or 4, 32)\n",
        "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "    results = list(tqdm(\n",
        "        executor.map(convert_to_wav, mp3_files),\n",
        "        total=len(mp3_files),\n",
        "        desc=\"Converting to WAV\",\n",
        "        unit=\" files\"\n",
        "    ))\n",
        "\n",
        "converted_count = sum(1 for r in results if r == \"success\")\n",
        "skipped_count = sum(1 for r in results if r == \"skip\")\n",
        "errors = [r[1] for r in results if isinstance(r, tuple) and r[0] == \"error\"]\n",
        "\n",
        "print(f\"Converted {converted_count} new files, {skipped_count} already existed\")\n",
        "if errors:\n",
        "    print(f\"WARNING: {len(errors)} files failed to convert (likely corrupted):\")\n",
        "    for err_file in errors[:10]:  # Show first 10 errors\n",
        "        print(f\"  ✗ {err_file}\")\n",
        "    if len(errors) > 10:\n",
        "        print(f\"  ... and {len(errors) - 10} more\")\n",
        "\n",
        "wav_files = sorted(WAV_DIR.glob(\"*.wav\"))\n",
        "print(\"WAV files ready:\", len(wav_files))\n",
        "for name in wav_files[:5]:\n",
        "    print(\" •\", name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ebc1933",
      "metadata": {},
      "source": [
        "## 8) Segment into 10s chunks\n",
        "Segments each WAV into fixed-duration clips.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "670b9843",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "SEGMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "wav_files = sorted(WAV_DIR.glob(\"*.wav\"))\n",
        "\n",
        "def segment_wav(wav):\n",
        "    \"\"\"Segment a single WAV file into fixed-duration clips\"\"\"\n",
        "    existing = list(SEGMENTS_DIR.glob(f\"{wav.stem}_seg_*.wav\"))\n",
        "    if existing:\n",
        "        return False\n",
        "    pattern = SEGMENTS_DIR / f\"{wav.stem}_seg_%03d.wav\"\n",
        "    cmd = [\n",
        "        \"ffmpeg\", \"-hide_banner\", \"-loglevel\", \"error\",\n",
        "        \"-i\", str(wav),\n",
        "        \"-f\", \"segment\",\n",
        "        \"-segment_time\", str(SEGMENT_SECONDS),\n",
        "        \"-reset_timestamps\", \"1\",\n",
        "        \"-map_metadata\", \"-1\",\n",
        "        str(pattern),\n",
        "    ]\n",
        "    subprocess.run(cmd, check=True)\n",
        "    return True\n",
        "\n",
        "# Use ThreadPoolExecutor for parallel segmentation\n",
        "max_workers = min(os.cpu_count() or 4, 32)\n",
        "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "    results = list(tqdm(\n",
        "        executor.map(segment_wav, wav_files),\n",
        "        total=len(wav_files),\n",
        "        desc=\"Segmenting WAVs\",\n",
        "        unit=\" files\"\n",
        "    ))\n",
        "\n",
        "segmented_count = sum(results)\n",
        "print(f\"Segmented {segmented_count} new files, {len(results) - segmented_count} already existed\")\n",
        "\n",
        "segments = sorted(SEGMENTS_DIR.glob(\"*.wav\"))\n",
        "print(\"Total segments:\", len(segments))\n",
        "\n",
        "def probe_duration(path: Path) -> float:\n",
        "    res = subprocess.run(\n",
        "        [\n",
        "            \"ffprobe\", \"-v\", \"error\",\n",
        "            \"-show_entries\", \"format=duration\",\n",
        "            \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n",
        "            str(path),\n",
        "        ],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        check=True,\n",
        "    )\n",
        "    return float(res.stdout.strip())\n",
        "\n",
        "sample_check = random.sample(segments, k=min(5, len(segments))) if segments else []\n",
        "for s in sample_check:\n",
        "    print(s.name, \"→\", round(probe_duration(s), 3), \"s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db14350",
      "metadata": {},
      "source": [
        "## 9) Create train/valid manifests\n",
        "Deterministic split using `RANDOM_SEED` and `TRAIN_RATIO`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32874f76",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "segments = sorted(SEGMENTS_DIR.glob(\"*.wav\"))\n",
        "random.seed(RANDOM_SEED)\n",
        "random.shuffle(segments)\n",
        "\n",
        "split_idx = int(len(segments) * TRAIN_RATIO)\n",
        "train_files = segments[:split_idx]\n",
        "valid_files = segments[split_idx:] or segments[-1:]\n",
        "\n",
        "MANIFEST_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def write_manifest(split_name, files):\n",
        "    \"\"\"Write a manifest file for a split\"\"\"\n",
        "    manifest_path = MANIFEST_DIR / f\"{split_name}.jsonl\"\n",
        "    with open(manifest_path, \"w\") as f:\n",
        "        for p in files:\n",
        "            f.write(json.dumps({\"path\": str(p)}) + \"\\n\")\n",
        "    return split_name, len(files)\n",
        "\n",
        "# Write both manifests in parallel\n",
        "with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "    futures = [\n",
        "        executor.submit(write_manifest, \"train\", train_files),\n",
        "        executor.submit(write_manifest, \"valid\", valid_files)\n",
        "    ]\n",
        "    results = [f.result() for f in futures]\n",
        "\n",
        "print(\"Train/valid counts:\", len(train_files), len(valid_files))\n",
        "print(\"Sample manifest line:\")\n",
        "print((MANIFEST_DIR / \"train.jsonl\").read_text().splitlines()[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a14c6ce",
      "metadata": {},
      "source": [
        "## 10) Create `egs/train` and `egs/valid` symlinks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c16040",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "for split, files in [(\"train\", train_files), (\"valid\", valid_files)]:\n",
        "    dest_dir = EGS_TRAIN if split == \"train\" else EGS_VALID\n",
        "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for path in files:\n",
        "        link = dest_dir / path.name\n",
        "        if link.exists() or link.is_symlink():\n",
        "            link.unlink()\n",
        "        link.symlink_to(path)\n",
        "\n",
        "print(\"egs/train count:\", len(list(EGS_TRAIN.glob(\"*.wav\"))))\n",
        "print(\"egs/valid count:\", len(list(EGS_VALID.glob(\"*.wav\"))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46fd84fb",
      "metadata": {},
      "source": [
        "## 11) Generate AudioCraft-native data.jsonl\n",
        "Adds `duration`, `sample_rate`, `channels` for each split.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77163f8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import subprocess\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# optional: where to log failures\n",
        "BAD_LOG = Path(\"ffprobe_missing_duration.log\")\n",
        "\n",
        "def probe(path: Path) -> dict:\n",
        "    res = subprocess.run(\n",
        "        [\n",
        "            \"ffprobe\", \"-v\", \"error\",\n",
        "            \"-select_streams\", \"a:0\",\n",
        "            \"-show_entries\", \"stream=sample_rate,channels:format=duration\",\n",
        "            \"-of\", \"json\",\n",
        "            str(path),\n",
        "        ],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "    )\n",
        "\n",
        "    if res.returncode != 0:\n",
        "        raise RuntimeError(f\"ffprobe failed: {path}\\n{res.stderr.strip()}\")\n",
        "\n",
        "    info = json.loads(res.stdout)\n",
        "    fmt = info.get(\"format\", {}) or {}\n",
        "    streams = info.get(\"streams\", []) or []\n",
        "\n",
        "    dur = fmt.get(\"duration\")\n",
        "    duration = float(dur) if dur not in (None, \"\", \"N/A\") else None\n",
        "\n",
        "    sr = streams[0].get(\"sample_rate\") if streams else None\n",
        "    sample_rate = int(sr) if sr not in (None, \"\", \"N/A\") else None\n",
        "\n",
        "    ch = streams[0].get(\"channels\") if streams else None\n",
        "    channels = int(ch) if ch not in (None, \"\", \"N/A\") else None\n",
        "\n",
        "    return {\"duration\": duration, \"sample_rate\": sample_rate, \"channels\": channels}\n",
        "\n",
        "def process_file(args):\n",
        "    path, egs_dir = args\n",
        "    payload = {\"path\": str(egs_dir / path.name)}\n",
        "    try:\n",
        "        payload.update(probe(path))\n",
        "    except Exception as e:\n",
        "        # log and keep going\n",
        "        with open(BAD_LOG, \"a\") as lf:\n",
        "            lf.write(f\"{path}\\t{repr(e)}\\n\")\n",
        "        payload.update({\"duration\": None, \"sample_rate\": None, \"channels\": None, \"error\": str(e)})\n",
        "    return payload\n",
        "\n",
        "for split, src_files, egs_dir in [\n",
        "    (\"train\", train_files, EGS_TRAIN),\n",
        "    (\"valid\", valid_files, EGS_VALID),\n",
        "]:\n",
        "    max_workers = min(os.cpu_count() or 4, 32)\n",
        "    probe_args = [(path, egs_dir) for path in src_files]\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        payloads = list(tqdm(\n",
        "            executor.map(process_file, probe_args),\n",
        "            total=len(probe_args),\n",
        "            desc=f\"Probing {split}\",\n",
        "            unit=\" files\"\n",
        "        ))\n",
        "\n",
        "    out_path = egs_dir / \"data.jsonl\"\n",
        "    with open(out_path, \"w\") as f:\n",
        "        for payload in payloads:\n",
        "            f.write(json.dumps(payload) + \"\\n\")\n",
        "\n",
        "    print(f\"Wrote {split} data.jsonl →\", out_path)\n",
        "    print(f\"Logged probe issues (if any) → {BAD_LOG}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f037def5",
      "metadata": {},
      "source": [
        "## 12) Create Hydra dataset config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c817a32e",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "mkdir -p /root/workspace/audiocraft/config/dset/audio\n",
        "cat > /root/workspace/audiocraft/config/dset/audio/all_data.yaml <<'YAML'\n",
        "# @package __global__\n",
        "\n",
        "datasource:\n",
        "  max_sample_rate: 32000\n",
        "  max_channels: 1\n",
        "  train: /root/workspace/data/all_data/egs/train\n",
        "  valid: /root/workspace/data/all_data/egs/valid\n",
        "  evaluate: /root/workspace/data/all_data/egs/valid\n",
        "  generate: /root/workspace/data/all_data/egs/valid\n",
        "YAML\n",
        "ls -l /root/workspace/audiocraft/config/dset/audio/all_data.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54a6332a",
      "metadata": {},
      "source": [
        "## 13) Ready-to-train checklist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b11ccba",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "print({\n",
        "    \"torch\": torch.__version__,\n",
        "    \"cuda\": torch.cuda.is_available(),\n",
        "    \"train_wavs\": len(list(EGS_TRAIN.glob(\"*.wav\"))),\n",
        "    \"valid_wavs\": len(list(EGS_VALID.glob(\"*.wav\"))),\n",
        "    \"train_jsonl\": EGS_TRAIN / \"data.jsonl\",\n",
        "    \"valid_jsonl\": EGS_VALID / \"data.jsonl\",\n",
        "    \"config\": Path('/root/workspace/audiocraft/config/dset/audio/all_data.yaml'),\n",
        "    \"experiments_dir\": EXPERIMENTS_DIR,\n",
        "})\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
