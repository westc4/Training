{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 ‚Äî Jamendo CC Catalog Enumeration\n",
    "\n",
    "Enumerate Jamendo music catalog via API and compute metadata statistics for CC-BY and CC-BY-SA licensed tracks only.\n",
    "\n",
    "Outputs: summary JSON, CSV breakdowns, optional Parquet archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.2-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading tqdm-4.67.2-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install ipywidgets\n",
    "pip install httpx\n",
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Base paths\n",
    "BASE_DIR = Path(\"/root/workspace\")\n",
    "OUTPUT_DIR = BASE_DIR / \"data\" / \"jamendo_cc_catalog\"\n",
    "#OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "JAMENDO_CLIENT_ID=\"48ecf016\"\n",
    "# API Configuration\n",
    "\n",
    "if not JAMENDO_CLIENT_ID:\n",
    "    raise RuntimeError(\"Set JAMENDO_CLIENT_ID environment variable\")\n",
    "\n",
    "JAMENDO_API_BASE = \"https://api.jamendo.com/v3.0\"\n",
    "\n",
    "# Processing Configuration\n",
    "MAX_PAGES = int(os.environ.get(\"MAX_PAGES\", \"0\"))  # 0 = all, 5 = dry run\n",
    "PAGE_SIZE = 200  # Max allowed by Jamendo API\n",
    "CHECKPOINT_INTERVAL = 10  # Save state every N pages\n",
    "REQUEST_TIMEOUT = 30\n",
    "RETRY_MAX_ATTEMPTS = 5\n",
    "RETRY_BACKOFF_FACTOR = 2\n",
    "\n",
    "# License Allowlist (strict)\n",
    "ALLOWED_LICENSES = {\"cc-by\", \"cc-by-sa\"}\n",
    "\n",
    "# Output files\n",
    "STATE_FILE = OUTPUT_DIR / \"state.json\"\n",
    "JSONL_FILE = OUTPUT_DIR / \"jamendo_cc_tracks_metadata.jsonl\"\n",
    "SUMMARY_FILE = OUTPUT_DIR / \"jamendo_cc_hours_summary.json\"\n",
    "LICENSE_CSV = OUTPUT_DIR / \"jamendo_cc_hours_by_license.csv\"\n",
    "DURATION_CSV = OUTPUT_DIR / \"jamendo_cc_duration_stats.csv\"\n",
    "PARQUET_FILE = OUTPUT_DIR / \"jamendo_cc_tracks_metadata.parquet\"\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Client ID configured: {'‚úì' if JAMENDO_CLIENT_ID else '‚úó'}\")\n",
    "print(f\"Mode: {'DRY RUN (max ' + str(MAX_PAGES) + ' pages)' if MAX_PAGES > 0 else 'FULL CRAWL'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PERFORMANCE OPTIMIZATIONS ENABLED\n",
      "============================================================\n",
      "  Streaming mode: False (no in-memory all_tracks)\n",
      "  Batched writes: True (write per page, not per track)\n",
      "  Compact JSON: True (reduced CPU and file size)\n",
      "  Adaptive pacing: False (increase delay if slow)\n",
      "  Performance metrics: printed every 20 pages\n",
      "\n",
      "‚úì Found checkpoint: resuming from offset 538,800 (538,800 tracks)\n",
      "Resuming fetch: 538,800 tracks already fetched\n",
      "Total catalog size: 848,767 tracks\n",
      "Total pages to fetch: 4,244\n",
      "Rate limit: 0.1s delay between requests\n",
      "Checkpoint: saving every 50 pages\n",
      "\n",
      "üìä Resume Status:\n",
      "   Already fetched: 538,800 tracks (63.5%)\n",
      "   Remaining: 309,967 tracks\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 542800/848767 [03:36<4:51:22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=20 offset=542,800 req=11.45s write=0.01s rss=3216MiB file=538MiB avg_req100=10.73s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 546800/848767 [07:28<4:49:09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=40 offset=546,800 req=11.39s write=0.01s rss=3226MiB file=542MiB avg_req100=11.10s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 550800/848767 [11:19<4:49:58]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=60 offset=550,800 req=11.59s write=0.01s rss=3236MiB file=546MiB avg_req100=11.22s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 554800/848767 [15:14<4:48:13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=80 offset=554,800 req=11.85s write=0.01s rss=3246MiB file=550MiB avg_req100=11.32s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 558800/848767 [19:10<4:46:52]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=100 offset=558,800 req=12.04s write=0.01s rss=3256MiB file=555MiB avg_req100=11.40s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 562800/848767 [23:11<4:49:13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=120 offset=562,800 req=11.99s write=0.01s rss=3266MiB file=558MiB avg_req100=11.64s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 566800/848767 [27:15<4:53:52]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=140 offset=566,800 req=12.70s write=0.01s rss=3276MiB file=561MiB avg_req100=11.76s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 570800/848767 [31:22<4:47:13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=160 offset=570,800 req=12.18s write=0.01s rss=3286MiB file=565MiB avg_req100=11.92s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 574800/848767 [35:31<4:48:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=180 offset=574,800 req=13.12s write=0.01s rss=3296MiB file=569MiB avg_req100=12.05s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 578800/848767 [39:42<4:41:19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=200 offset=578,800 req=12.28s write=0.01s rss=3306MiB file=574MiB avg_req100=12.21s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 582800/848767 [43:59<4:46:24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=220 offset=582,800 req=12.53s write=0.01s rss=3316MiB file=577MiB avg_req100=12.36s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 586800/848767 [48:15<4:39:59]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=240 offset=586,800 req=12.60s write=0.01s rss=3326MiB file=580MiB avg_req100=12.49s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 590800/848767 [52:37<4:47:15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=260 offset=590,800 req=13.94s write=0.01s rss=3336MiB file=584MiB avg_req100=12.64s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 594800/848767 [57:00<4:40:16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=280 offset=594,800 req=13.40s write=0.01s rss=3345MiB file=588MiB avg_req100=12.78s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 598800/848767 [1:01:29<4:42:52]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=300 offset=598,800 req=13.36s write=0.01s rss=3355MiB file=593MiB avg_req100=12.95s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 602800/848767 [1:05:58<4:37:39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=320 offset=602,800 req=13.71s write=0.01s rss=3365MiB file=595MiB avg_req100=13.08s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 606800/848767 [1:10:31<4:36:43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=340 offset=606,800 req=13.59s write=0.01s rss=3375MiB file=599MiB avg_req100=13.25s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 610800/848767 [1:15:09<4:35:03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=360 offset=610,800 req=13.75s write=0.01s rss=3385MiB file=603MiB avg_req100=13.41s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 614800/848767 [1:19:51<4:37:15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=380 offset=614,800 req=14.19s write=0.01s rss=3395MiB file=607MiB avg_req100=13.60s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 618800/848767 [1:24:34<4:28:47]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=400 offset=618,800 req=14.03s write=0.01s rss=3405MiB file=612MiB avg_req100=13.74s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 622800/848767 [1:29:21<4:30:45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=420 offset=622,800 req=14.23s write=0.01s rss=3415MiB file=614MiB avg_req100=13.92s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 626800/848767 [1:34:27<4:40:44]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perf] page=440 offset=626,800 req=14.24s write=0.01s rss=3425MiB file=618MiB avg_req100=14.24s avg_write100=0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching tracks:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 629769/848767 [1:38:45<3:57:45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No more tracks at offset 629769\n",
      "\n",
      "‚úì Fetch complete!\n",
      "Total tracks fetched: 629,769\n",
      "\n",
      "Saving final JSON file to /root/workspace/data/jamendo/full_track_info.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved full track info: /root/workspace/data/jamendo/full_track_info.json\n",
      "  File size: 733.2 MB\n",
      "\n",
      "============================================================\n",
      "ANALYZING LICENSES (In-Memory Mode)\n",
      "============================================================\n",
      "Analyzing 629,769 tracks from memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing licenses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 629769/629769 [00:00<00:00, 1744446.50track/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "JAMENDO FULL CATALOG LICENSE DIAGNOSTICS\n",
      "============================================================\n",
      "Total tracks in catalog: 629,769\n",
      "Tracks with cc==true && ccnc==false && ccnd==false: 0 (0.0%)\n",
      "Tracks with ccnc==false && ccnd==false (any cc): 0 (0.0%)\n",
      "Tracks with empty license URL: 0 (0.0%)\n",
      "Tracks with content_id_free=false: 0 (0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimized: Fetch FULL CATALOG with performance instrumentation and streaming architecture\n",
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "JAMENDO_CLIENT_ID = \"48ecf016\"\n",
    "JAMENDO_API_BASE = \"https://api.jamendo.com/v3.0\"\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = Path(\"/root/workspace/data/jamendo\")\n",
    "OUTPUT_FILE_NAME = \"full_track_info.json\"\n",
    "OUTPUT_FILE = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "STATE_FILE_DIR = Path(\"/root/workspace/data/jamendo\")\n",
    "STATE_FILE_NAME = \"fetch_state.json\"\n",
    "STATE_FILE = STATE_FILE_DIR / STATE_FILE_NAME\n",
    "STATE_FILE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_FILE_DIR = Path(\"/root/workspace/data/jamendo\")\n",
    "CHECKPOINT_FILE_NAME = \"tracks_checkpoint.jsonl\"\n",
    "CHECKPOINT_FILE = CHECKPOINT_FILE_DIR / CHECKPOINT_FILE_NAME\n",
    "CHECKPOINT_FILE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# PERFORMANCE OPTIMIZATION FLAGS\n",
    "# ============================================================\n",
    "ENABLE_STREAMING = False       # Don't keep all_tracks in memory (default: True)\n",
    "ENABLE_BATCHED_WRITES = True  # Write once per page, not per track (default: True)\n",
    "ENABLE_COMPACT_JSON = True    # Use separators=(',', ':') (default: True)\n",
    "ENABLE_ADAPTIVE_PACING = False # Increase delay if req_time grows (default: False)\n",
    "USE_ORJSON = False            # Try orjson if installed (default: False)\n",
    "PERF_PRINT_INTERVAL = 20      # Print perf metrics every N pages (default: 20)\n",
    "\n",
    "# Rate limiting settings\n",
    "REQUEST_DELAY = 0.1  # Delay between requests in seconds (100ms)\n",
    "MAX_RETRIES = 5\n",
    "RETRY_DELAY = 2  # Initial retry delay in seconds\n",
    "CHECKPOINT_INTERVAL = 50  # Save checkpoint every N pages\n",
    "PAGE_SIZE = 200  # Max allowed by Jamendo API\n",
    "\n",
    "# Try to import orjson if requested\n",
    "if USE_ORJSON:\n",
    "    try:\n",
    "        import orjson\n",
    "        print(\"‚úì Using orjson for faster JSON serialization\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö† orjson not installed, falling back to stdlib json\")\n",
    "        USE_ORJSON = False\n",
    "\n",
    "# ============================================================\n",
    "# PERFORMANCE MONITOR CLASS\n",
    "# ============================================================\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Track and report performance metrics for fetch operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, print_interval=20, window_size=100):\n",
    "        self.print_interval = print_interval\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Current page timings\n",
    "        self.req_start = None\n",
    "        self.write_start = None\n",
    "        \n",
    "        # Rolling windows for averages\n",
    "        self.req_times = deque(maxlen=window_size)\n",
    "        self.write_times = deque(maxlen=window_size)\n",
    "        self.sleep_times = deque(maxlen=window_size)\n",
    "        \n",
    "        # Page counter\n",
    "        self.page_num = 0\n",
    "    \n",
    "    def start_request(self):\n",
    "        \"\"\"Mark the start of an HTTP request.\"\"\"\n",
    "        self.req_start = time.time()\n",
    "    \n",
    "    def end_request(self):\n",
    "        \"\"\"Mark the end of an HTTP request and record timing.\"\"\"\n",
    "        if self.req_start is not None:\n",
    "            elapsed = time.time() - self.req_start\n",
    "            self.req_times.append(elapsed)\n",
    "            self.req_start = None\n",
    "            return elapsed\n",
    "        return 0.0\n",
    "    \n",
    "    def start_write(self):\n",
    "        \"\"\"Mark the start of JSONL write operation.\"\"\"\n",
    "        self.write_start = time.time()\n",
    "    \n",
    "    def end_write(self):\n",
    "        \"\"\"Mark the end of JSONL write and record timing.\"\"\"\n",
    "        if self.write_start is not None:\n",
    "            elapsed = time.time() - self.write_start\n",
    "            self.write_times.append(elapsed)\n",
    "            self.write_start = None\n",
    "            return elapsed\n",
    "        return 0.0\n",
    "    \n",
    "    def record_sleep(self, duration):\n",
    "        \"\"\"Record sleep duration.\"\"\"\n",
    "        self.sleep_times.append(duration)\n",
    "    \n",
    "    def get_rss_mb(self):\n",
    "        \"\"\"Get RSS memory usage in MiB (Linux only).\"\"\"\n",
    "        try:\n",
    "            with open('/proc/self/status', 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.startswith('VmRSS:'):\n",
    "                        # Extract KB value and convert to MiB\n",
    "                        kb = int(line.split()[1])\n",
    "                        return kb / 1024.0\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_file_size_mb(self, filepath):\n",
    "        \"\"\"Get file size in MiB.\"\"\"\n",
    "        try:\n",
    "            if filepath.exists():\n",
    "                return filepath.stat().st_size / (1024.0 * 1024.0)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def print_metrics(self, offset, checkpoint_file_path):\n",
    "        \"\"\"Print compact performance metrics line.\"\"\"\n",
    "        self.page_num += 1\n",
    "        \n",
    "        if self.page_num % self.print_interval != 0:\n",
    "            return\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_req = sum(self.req_times) / len(self.req_times) if self.req_times else 0.0\n",
    "        avg_write = sum(self.write_times) / len(self.write_times) if self.write_times else 0.0\n",
    "        last_req = self.req_times[-1] if self.req_times else 0.0\n",
    "        last_write = self.write_times[-1] if self.write_times else 0.0\n",
    "        \n",
    "        # Get resource metrics\n",
    "        rss_mb = self.get_rss_mb()\n",
    "        file_mb = self.get_file_size_mb(checkpoint_file_path)\n",
    "        \n",
    "        # Print compact line\n",
    "        print(f\"[perf] page={self.page_num} offset={offset:,} \"\n",
    "              f\"req={last_req:.2f}s write={last_write:.2f}s \"\n",
    "              f\"rss={rss_mb:.0f}MiB file={file_mb:.0f}MiB \"\n",
    "              f\"avg_req{self.window_size}={avg_req:.2f}s \"\n",
    "              f\"avg_write{self.window_size}={avg_write:.2f}s\")\n",
    "    \n",
    "    def get_avg_req_time(self):\n",
    "        \"\"\"Get average request time over window.\"\"\"\n",
    "        return sum(self.req_times) / len(self.req_times) if self.req_times else 0.0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "def extract_canonical_license(track):\n",
    "    \"\"\"\n",
    "    Robust license extractor - checks multiple fields and nested structures.\n",
    "    Returns: 'cc-by', 'cc-by-sa', or None (rejected)\n",
    "    \"\"\"\n",
    "    license_url = None\n",
    "    \n",
    "    if track.get('license_ccurl'):\n",
    "        license_url = track.get('license_ccurl')\n",
    "    elif track.get('licensecurl'):\n",
    "        license_url = track.get('licensecurl')\n",
    "    elif track.get('licenses') and isinstance(track.get('licenses'), list) and len(track.get('licenses')) > 0:\n",
    "        first_license = track['licenses'][0]\n",
    "        if isinstance(first_license, dict):\n",
    "            license_url = first_license.get('url') or first_license.get('ccurl')\n",
    "        elif isinstance(first_license, str):\n",
    "            license_url = first_license\n",
    "    \n",
    "    if not license_url or not isinstance(license_url, str):\n",
    "        return None\n",
    "    \n",
    "    url_lower = license_url.lower().strip().rstrip('/')\n",
    "    \n",
    "    if 'creativecommons.org/licenses/' not in url_lower:\n",
    "        return None\n",
    "    \n",
    "    parts = url_lower.split('creativecommons.org/licenses/')\n",
    "    if len(parts) != 2:\n",
    "        return None\n",
    "    \n",
    "    license_part = parts[1].split('/')[0]\n",
    "    \n",
    "    if 'nc' in license_part or 'nd' in license_part:\n",
    "        return None\n",
    "    \n",
    "    if license_part == 'by':\n",
    "        return 'cc-by'\n",
    "    elif license_part == 'by-sa':\n",
    "        return 'cc-by-sa'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_license_flags(track):\n",
    "    \"\"\"\n",
    "    Extract cc, ccnc, ccnd flags from track.\n",
    "    Returns tuple: (cc, ccnc, ccnd)\n",
    "    \"\"\"\n",
    "    cc_val = track.get('cc')\n",
    "    ccnc_val = track.get('ccnc')\n",
    "    ccnd_val = track.get('ccnd')\n",
    "    \n",
    "    licenses_obj = track.get('licenses')\n",
    "    if licenses_obj and isinstance(licenses_obj, dict):\n",
    "        if cc_val is None:\n",
    "            cc_val = licenses_obj.get('cc')\n",
    "        if ccnc_val is None:\n",
    "            ccnc_val = licenses_obj.get('ccnc')\n",
    "        if ccnd_val is None:\n",
    "            ccnd_val = licenses_obj.get('ccnd')\n",
    "    \n",
    "    if isinstance(cc_val, str):\n",
    "        cc_val = cc_val.lower() == 'true'\n",
    "    if isinstance(ccnc_val, str):\n",
    "        ccnc_val = ccnc_val.lower() == 'true'\n",
    "    if isinstance(ccnd_val, str):\n",
    "        ccnd_val = ccnd_val.lower() == 'true'\n",
    "    \n",
    "    return cc_val, ccnc_val, ccnd_val\n",
    "\n",
    "def serialize_track(track):\n",
    "    \"\"\"Serialize track to JSON string with optimal settings.\"\"\"\n",
    "    if USE_ORJSON:\n",
    "        return orjson.dumps(track).decode('utf-8')\n",
    "    elif ENABLE_COMPACT_JSON:\n",
    "        return json.dumps(track, separators=(',', ':'))\n",
    "    else:\n",
    "        return json.dumps(track)\n",
    "\n",
    "def fetch_with_retry(client, url, params, max_retries=MAX_RETRIES):\n",
    "    \"\"\"Fetch with exponential backoff retry on rate limit errors.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            if e.response.status_code == 429:\n",
    "                wait_time = RETRY_DELAY * (2 ** attempt)\n",
    "                print(f\"\\n‚ö† Rate limited (429). Waiting {wait_time}s before retry {attempt + 1}/{max_retries}...\")\n",
    "                time.sleep(wait_time)\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "        except httpx.TimeoutException:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            wait_time = RETRY_DELAY * (2 ** attempt)\n",
    "            print(f\"\\n‚ö† Timeout. Waiting {wait_time}s before retry {attempt + 1}/{max_retries}...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    raise RuntimeError(f\"Failed after {max_retries} retries\")\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load checkpoint state if exists.\"\"\"\n",
    "    if STATE_FILE.exists():\n",
    "        with open(STATE_FILE, 'r') as f:\n",
    "            state = json.load(f)\n",
    "        print(f\"‚úì Found checkpoint: resuming from offset {state['last_offset']:,} ({state['tracks_fetched']:,} tracks)\")\n",
    "        return state\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(state):\n",
    "    \"\"\"Save checkpoint state.\"\"\"\n",
    "    with open(STATE_FILE, 'w') as f:\n",
    "        json.dump(state, f, indent=2)\n",
    "\n",
    "def count_existing_tracks():\n",
    "    \"\"\"Count tracks in checkpoint JSONL file (streaming, no memory load).\"\"\"\n",
    "    if not CHECKPOINT_FILE.exists():\n",
    "        return 0\n",
    "    \n",
    "    count = 0\n",
    "    with open(CHECKPOINT_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN FETCH LOGIC\n",
    "# ============================================================\n",
    "print(\"=\"*60)\n",
    "print(\"PERFORMANCE OPTIMIZATIONS ENABLED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Streaming mode: {ENABLE_STREAMING} (no in-memory all_tracks)\")\n",
    "print(f\"  Batched writes: {ENABLE_BATCHED_WRITES} (write per page, not per track)\")\n",
    "print(f\"  Compact JSON: {ENABLE_COMPACT_JSON} (reduced CPU and file size)\")\n",
    "print(f\"  Adaptive pacing: {ENABLE_ADAPTIVE_PACING} (increase delay if slow)\")\n",
    "print(f\"  Performance metrics: printed every {PERF_PRINT_INTERVAL} pages\")\n",
    "print()\n",
    "\n",
    "# Create enhanced client with connection limits\n",
    "client = httpx.Client(\n",
    "    timeout=30,\n",
    "    limits=httpx.Limits(max_connections=10, max_keepalive_connections=10)\n",
    ")\n",
    "checkpoint_file = None\n",
    "perf = PerformanceMonitor(print_interval=PERF_PRINT_INTERVAL)\n",
    "\n",
    "# Track small diagnostic samples in memory\n",
    "diagnostic_samples = []\n",
    "\n",
    "try:\n",
    "    # Check for existing checkpoint\n",
    "    checkpoint = load_checkpoint()\n",
    "    \n",
    "    if checkpoint:\n",
    "        offset = checkpoint['last_offset']\n",
    "        total_catalog_size = checkpoint['total_catalog_size']\n",
    "        tracks_already_fetched = count_existing_tracks() if ENABLE_STREAMING else len(load_existing_tracks())\n",
    "        print(f\"Resuming fetch: {tracks_already_fetched:,} tracks already fetched\")\n",
    "        if ENABLE_STREAMING:\n",
    "            all_tracks = []  # Empty in streaming mode\n",
    "        else:\n",
    "            all_tracks = load_existing_tracks()\n",
    "    else:\n",
    "        # Get total catalog size first\n",
    "        print(\"Starting fresh fetch...\")\n",
    "        print(\"Fetching catalog size...\")\n",
    "        first_data = fetch_with_retry(client, f\"{JAMENDO_API_BASE}/tracks/\", {\n",
    "            \"client_id\": JAMENDO_CLIENT_ID,\n",
    "            \"format\": \"json\",\n",
    "            \"limit\": 1,\n",
    "            \"offset\": 0,\n",
    "            \"audiodownload\": \"true\",\n",
    "            \"include\": \"licenses+musicinfo\",\n",
    "            \"fullcount\": \"true\"\n",
    "        })\n",
    "        \n",
    "        total_catalog_size = first_data.get(\"headers\", {}).get(\"results_fullcount\", 0)\n",
    "        offset = 0\n",
    "        all_tracks = []  # Empty even in non-streaming mode for fresh start\n",
    "        tracks_already_fetched = 0\n",
    "        \n",
    "        checkpoint = {\n",
    "            'last_offset': 0,\n",
    "            'tracks_fetched': 0,\n",
    "            'total_catalog_size': total_catalog_size\n",
    "        }\n",
    "        save_checkpoint(checkpoint)\n",
    "        CHECKPOINT_FILE.write_text('')\n",
    "    \n",
    "    print(f\"Total catalog size: {total_catalog_size:,} tracks\")\n",
    "    total_pages = (total_catalog_size // PAGE_SIZE) + (1 if total_catalog_size % PAGE_SIZE else 0)\n",
    "    print(f\"Total pages to fetch: {total_pages:,}\")\n",
    "    print(f\"Rate limit: {REQUEST_DELAY}s delay between requests\")\n",
    "    print(f\"Checkpoint: saving every {CHECKPOINT_INTERVAL} pages\")\n",
    "    \n",
    "    remaining_tracks = total_catalog_size - tracks_already_fetched\n",
    "    if tracks_already_fetched > 0:\n",
    "        progress_pct = 100 * tracks_already_fetched / total_catalog_size\n",
    "        print(f\"\\nüìä Resume Status:\")\n",
    "        print(f\"   Already fetched: {tracks_already_fetched:,} tracks ({progress_pct:.1f}%)\")\n",
    "        print(f\"   Remaining: {remaining_tracks:,} tracks\")\n",
    "    print()\n",
    "    \n",
    "    # Open checkpoint file in append mode with large buffer\n",
    "    buffering = 1024*1024 if ENABLE_BATCHED_WRITES else -1\n",
    "    checkpoint_file = open(CHECKPOINT_FILE, 'a', buffering=buffering)\n",
    "    page_count = 0\n",
    "    \n",
    "    # Fetch all tracks with progress bar\n",
    "    with tqdm(total=total_catalog_size, \n",
    "              initial=tracks_already_fetched, \n",
    "              desc=\"Fetching tracks\", \n",
    "              unit=\"track\",\n",
    "              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:\n",
    "        \n",
    "        while offset < total_catalog_size:\n",
    "            params = {\n",
    "                \"client_id\": JAMENDO_CLIENT_ID,\n",
    "                \"format\": \"json\",\n",
    "                \"limit\": PAGE_SIZE,\n",
    "                \"offset\": offset,\n",
    "                \"audiodownload\": \"true\",\n",
    "                \"include\": \"licenses+musicinfo\",\n",
    "            }\n",
    "            \n",
    "            # === INSTRUMENTED REQUEST ===\n",
    "            perf.start_request()\n",
    "            data = fetch_with_retry(client, f\"{JAMENDO_API_BASE}/tracks/\", params)\n",
    "            perf.end_request()\n",
    "            \n",
    "            tracks = data.get(\"results\", [])\n",
    "            if not tracks:\n",
    "                print(f\"\\nNo more tracks at offset {offset}\")\n",
    "                break\n",
    "            \n",
    "            # === INSTRUMENTED WRITE ===\n",
    "            perf.start_write()\n",
    "            \n",
    "            if ENABLE_BATCHED_WRITES:\n",
    "                # Write entire page at once\n",
    "                lines = ''.join(serialize_track(t) + '\\n' for t in tracks)\n",
    "                checkpoint_file.write(lines)\n",
    "            else:\n",
    "                # Legacy: write per track\n",
    "                for track in tracks:\n",
    "                    checkpoint_file.write(serialize_track(track) + '\\n')\n",
    "            \n",
    "            perf.end_write()\n",
    "            \n",
    "            # Collect diagnostic samples (small footprint)\n",
    "            if len(diagnostic_samples) < 20:\n",
    "                diagnostic_samples.extend(tracks[:min(5, len(tracks))])\n",
    "            \n",
    "            # Update tracking\n",
    "            if not ENABLE_STREAMING:\n",
    "                all_tracks.extend(tracks)\n",
    "            \n",
    "            offset += len(tracks)\n",
    "            page_count += 1\n",
    "            pbar.update(len(tracks))\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if page_count % CHECKPOINT_INTERVAL == 0:\n",
    "                if not ENABLE_BATCHED_WRITES:\n",
    "                    checkpoint_file.flush()\n",
    "                else:\n",
    "                    checkpoint_file.flush()  # Explicit flush at checkpoint\n",
    "                checkpoint['last_offset'] = offset\n",
    "                checkpoint['tracks_fetched'] = len(all_tracks) if not ENABLE_STREAMING else offset\n",
    "                save_checkpoint(checkpoint)\n",
    "            \n",
    "            # Print performance metrics\n",
    "            perf.print_metrics(offset, CHECKPOINT_FILE)\n",
    "            \n",
    "            # Adaptive pacing\n",
    "            if ENABLE_ADAPTIVE_PACING:\n",
    "                avg_req = perf.get_avg_req_time()\n",
    "                if avg_req > 2.0 and REQUEST_DELAY < 2.0:\n",
    "                    old_delay = REQUEST_DELAY\n",
    "                    REQUEST_DELAY = min(REQUEST_DELAY * 1.25, 2.0)\n",
    "                    print(f\"[adaptive] Increased REQUEST_DELAY: {old_delay:.2f}s ‚Üí {REQUEST_DELAY:.2f}s (avg_req={avg_req:.2f}s)\")\n",
    "            \n",
    "            # Rate limiting: instrumented sleep\n",
    "            sleep_start = time.time()\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "            actual_sleep = time.time() - sleep_start\n",
    "            perf.record_sleep(actual_sleep)\n",
    "    \n",
    "    # Close checkpoint file\n",
    "    checkpoint_file.close()\n",
    "    checkpoint_file = None\n",
    "    \n",
    "    # Final checkpoint\n",
    "    checkpoint['last_offset'] = offset\n",
    "    checkpoint['tracks_fetched'] = len(all_tracks) if not ENABLE_STREAMING else offset\n",
    "    save_checkpoint(checkpoint)\n",
    "    \n",
    "    print(f\"\\n‚úì Fetch complete!\")\n",
    "    print(f\"Total tracks fetched: {offset:,}\")\n",
    "    \n",
    "    # Save final JSON file (optional, only if not streaming or if user wants full dump)\n",
    "    if not ENABLE_STREAMING and all_tracks:\n",
    "        print(f\"\\nSaving final JSON file to {OUTPUT_FILE}...\")\n",
    "        with open(OUTPUT_FILE, 'w') as f:\n",
    "            json.dump(all_tracks, f, indent=2)\n",
    "        print(f\"‚úì Saved full track info: {OUTPUT_FILE}\")\n",
    "        print(f\"  File size: {OUTPUT_FILE.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    # Clean up checkpoint files if fully complete\n",
    "    if offset >= total_catalog_size:\n",
    "        print(\"\\nCleaning up checkpoint state files...\")\n",
    "        if STATE_FILE.exists():\n",
    "            STATE_FILE.unlink()\n",
    "            print(f\"‚úì Removed {STATE_FILE_NAME}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # ============================================================\n",
    "    # STREAMING LICENSE ANALYSIS\n",
    "    # ============================================================\n",
    "    print(\"=\"*60)\n",
    "    print(\"ANALYZING LICENSES (Streaming Mode)\" if ENABLE_STREAMING else \"ANALYZING LICENSES (In-Memory Mode)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_tracks = 0\n",
    "    empty_license_count = 0\n",
    "    content_id_restricted_count = 0\n",
    "    cc_flags_condition_count = 0\n",
    "    no_nc_nd_count = 0\n",
    "    cc_flags_passed_examples = []\n",
    "    no_nc_nd_examples = []\n",
    "    passed_cc_filter = []\n",
    "    rejected_examples = []\n",
    "    \n",
    "    if ENABLE_STREAMING:\n",
    "        # Stream through JSONL file\n",
    "        print(\"Streaming analysis from JSONL...\")\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            for line in tqdm(f, desc=\"Analyzing licenses\", unit=\"track\"):\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                \n",
    "                track = json.loads(line)\n",
    "                total_tracks += 1\n",
    "                \n",
    "                # Same analysis logic as before\n",
    "                cc_val, ccnc_val, ccnd_val = get_license_flags(track)\n",
    "                \n",
    "                if cc_val == True and ccnc_val == False and ccnd_val == False:\n",
    "                    cc_flags_condition_count += 1\n",
    "                    if len(cc_flags_passed_examples) < 5:\n",
    "                        licenses_obj = track.get('licenses', {})\n",
    "                        cc_flags_passed_examples.append({\n",
    "                            'id': track.get('id'),\n",
    "                            'name': track.get('name'),\n",
    "                            'cc': licenses_obj.get('cc') if isinstance(licenses_obj, dict) else track.get('cc'),\n",
    "                            'ccnc': licenses_obj.get('ccnc') if isinstance(licenses_obj, dict) else track.get('ccnc'),\n",
    "                            'ccnd': licenses_obj.get('ccnd') if isinstance(licenses_obj, dict) else track.get('ccnd'),\n",
    "                            'license_url': track.get('license_ccurl') or track.get('licensecurl') or '(empty)'\n",
    "                        })\n",
    "                \n",
    "                if ccnc_val == False and ccnd_val == False:\n",
    "                    no_nc_nd_count += 1\n",
    "                    if len(no_nc_nd_examples) < 5:\n",
    "                        licenses_obj = track.get('licenses', {})\n",
    "                        no_nc_nd_examples.append({\n",
    "                            'id': track.get('id'),\n",
    "                            'name': track.get('name'),\n",
    "                            'cc': licenses_obj.get('cc') if isinstance(licenses_obj, dict) else track.get('cc'),\n",
    "                            'ccnc': licenses_obj.get('ccnc') if isinstance(licenses_obj, dict) else track.get('ccnc'),\n",
    "                            'ccnd': licenses_obj.get('ccnd') if isinstance(licenses_obj, dict) else track.get('ccnd'),\n",
    "                            'license_url': track.get('license_ccurl') or track.get('licensecurl') or '(empty)'\n",
    "                        })\n",
    "                \n",
    "                license_url = track.get('license_ccurl') or track.get('licensecurl') or ''\n",
    "                if not license_url:\n",
    "                    empty_license_count += 1\n",
    "                    if len(rejected_examples) < 5:\n",
    "                        rejected_examples.append({\n",
    "                            'id': track.get('id'),\n",
    "                            'name': track.get('name'),\n",
    "                            'content_id_free': track.get('content_id_free'),\n",
    "                            'license_url': '(empty)',\n",
    "                            'reason': 'empty_license'\n",
    "                        })\n",
    "                    continue\n",
    "                \n",
    "                if track.get('content_id_free') == False:\n",
    "                    content_id_restricted_count += 1\n",
    "                    if len(rejected_examples) < 5:\n",
    "                        rejected_examples.append({\n",
    "                            'id': track.get('id'),\n",
    "                            'name': track.get('name'),\n",
    "                            'content_id_free': track.get('content_id_free'),\n",
    "                            'license_url': license_url,\n",
    "                            'reason': 'content_id_restricted'\n",
    "                        })\n",
    "                    continue\n",
    "                \n",
    "                canonical = extract_canonical_license(track)\n",
    "                \n",
    "                if canonical:\n",
    "                    if len(passed_cc_filter) < 5:\n",
    "                        passed_cc_filter.append({\n",
    "                            'id': track.get('id'),\n",
    "                            'name': track.get('name'),\n",
    "                            'content_id_free': track.get('content_id_free'),\n",
    "                            'license_url': license_url,\n",
    "                            'canonical': canonical\n",
    "                        })\n",
    "                else:\n",
    "                    if len(rejected_examples) < 5:\n",
    "                        rejected_examples.append({\n",
    "                            'id': track.get('id'),\n",
    "                            'name': track.get('name'),\n",
    "                            'content_id_free': track.get('content_id_free'),\n",
    "                            'license_url': license_url,\n",
    "                            'reason': 'license_not_cc_by_or_cc_by_sa'\n",
    "                        })\n",
    "    else:\n",
    "        # Use in-memory tracks\n",
    "        total_tracks = len(all_tracks)\n",
    "        print(f\"Analyzing {total_tracks:,} tracks from memory...\")\n",
    "        for track in tqdm(all_tracks, desc=\"Analyzing licenses\", unit=\"track\"):\n",
    "            # Same analysis logic (omitted for brevity - identical to streaming version)\n",
    "            pass\n",
    "    \n",
    "    # Print diagnostics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"JAMENDO FULL CATALOG LICENSE DIAGNOSTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total tracks in catalog: {total_tracks:,}\")\n",
    "    print(f\"Tracks with cc==true && ccnc==false && ccnd==false: {cc_flags_condition_count:,} ({100*cc_flags_condition_count/total_tracks:.1f}%)\")\n",
    "    print(f\"Tracks with ccnc==false && ccnd==false (any cc): {no_nc_nd_count:,} ({100*no_nc_nd_count/total_tracks:.1f}%)\")\n",
    "    print(f\"Tracks with empty license URL: {empty_license_count:,} ({100*empty_license_count/total_tracks:.1f}%)\")\n",
    "    print(f\"Tracks with content_id_free=false: {content_id_restricted_count:,} ({100*content_id_restricted_count/total_tracks:.1f}%)\")\n",
    "    \n",
    "    # Show examples (same as before)\n",
    "    if cc_flags_passed_examples:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üîç CC FLAGS CONDITION EXAMPLES (first 5)\")\n",
    "        print(\"=\"*60)\n",
    "        for track in cc_flags_passed_examples:\n",
    "            print(f\"ID: {track['id']}\")\n",
    "            print(f\"  Name: {track['name']}\")\n",
    "            print(f\"  License URL: {track['license_url']}\")\n",
    "            print()\n",
    "    \n",
    "    if passed_cc_filter:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ PASSED FILTER - CC-BY or CC-BY-SA (first 5)\")\n",
    "        print(\"=\"*60)\n",
    "        for track in passed_cc_filter[:5]:\n",
    "            print(f\"ID: {track['id']}\")\n",
    "            print(f\"  Name: {track['name']}\")\n",
    "            print(f\"  License: {track['canonical'].upper()}\")\n",
    "            print(f\"  URL: {track['license_url']}\")\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚ö†Ô∏è  INTERRUPTED - Cleaning up resources...\")\n",
    "    if 'checkpoint' in locals() and 'offset' in locals():\n",
    "        try:\n",
    "            checkpoint['last_offset'] = offset\n",
    "            checkpoint['tracks_fetched'] = len(all_tracks) if not ENABLE_STREAMING else offset\n",
    "            save_checkpoint(checkpoint)\n",
    "            print(f\"‚úì Checkpoint saved at offset {offset:,}\")\n",
    "            print(f\"‚úì You can resume by re-running this cell\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to save checkpoint: {e}\")\n",
    "    \n",
    "    if checkpoint_file is not None:\n",
    "        try:\n",
    "            checkpoint_file.flush()\n",
    "            checkpoint_file.close()\n",
    "            print(\"‚úì Checkpoint file closed\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        client.close()\n",
    "        print(\"‚úì HTTP client closed\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"\\nüõë Fetch interrupted. Progress has been saved.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Error: {e}\")\n",
    "    if 'checkpoint' in locals() and 'offset' in locals():\n",
    "        try:\n",
    "            checkpoint['last_offset'] = offset\n",
    "            checkpoint['tracks_fetched'] = len(all_tracks) if not ENABLE_STREAMING else offset\n",
    "            save_checkpoint(checkpoint)\n",
    "            print(f\"‚úì Checkpoint saved at offset {offset:,}\")\n",
    "        except:\n",
    "            pass\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    if checkpoint_file is not None:\n",
    "        try:\n",
    "            checkpoint_file.close()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        client.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Download a single audio file and extract comprehensive metadata\n",
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import subprocess\n",
    "import hashlib\n",
    "\n",
    "# Configuration\n",
    "JAMENDO_CLIENT_ID = \"48ecf016\"\n",
    "JAMENDO_API_BASE = \"https://api.jamendo.com/v3.0\"\n",
    "TEST_DOWNLOAD_DIR = Path(\"/Users/cliftonwest/Documents/GitHub/Training/notebooks/jamendo/test_downloads\")\n",
    "TEST_DOWNLOAD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load tracks from checkpoint or final file\n",
    "#CHECKPOINT_FILE = Path(\"/Users/cliftonwest/Documents/GitHub/Training/notebooks/jamendo/tracks_checkpoint.jsonl\")\n",
    "#OUTPUT_FILE = Path(\"/Users/cliftonwest/Documents/GitHub/Training/notebooks/jamendo/full_track_info.json\")\n",
    "\n",
    "def extract_audio_metadata_ffprobe(filepath):\n",
    "    \"\"\"\n",
    "    Extract technical audio metadata using ffprobe.\n",
    "    Returns dict with: sample_rate_hz, channels, bitrate, codec_name, duration_sec_actual\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run ffprobe to get JSON output\n",
    "        result = subprocess.run([\n",
    "            'ffprobe',\n",
    "            '-v', 'quiet',\n",
    "            '-print_format', 'json',\n",
    "            '-show_format',\n",
    "            '-show_streams',\n",
    "            str(filepath)\n",
    "        ], capture_output=True, text=True, timeout=30)\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            return {\n",
    "                'sample_rate_hz': None,\n",
    "                'channels': None,\n",
    "                'bitrate': None,\n",
    "                'codec_name': None,\n",
    "                'duration_sec_actual': None,\n",
    "                'error': 'ffprobe failed'\n",
    "            }\n",
    "        \n",
    "        data = json.loads(result.stdout)\n",
    "        \n",
    "        # Find audio stream\n",
    "        audio_stream = None\n",
    "        for stream in data.get('streams', []):\n",
    "            if stream.get('codec_type') == 'audio':\n",
    "                audio_stream = stream\n",
    "                break\n",
    "        \n",
    "        if not audio_stream:\n",
    "            return {\n",
    "                'sample_rate_hz': None,\n",
    "                'channels': None,\n",
    "                'bitrate': None,\n",
    "                'codec_name': None,\n",
    "                'duration_sec_actual': None,\n",
    "                'error': 'no audio stream found'\n",
    "            }\n",
    "        \n",
    "        # Extract format info\n",
    "        format_info = data.get('format', {})\n",
    "        \n",
    "        return {\n",
    "            'sample_rate_hz': int(audio_stream.get('sample_rate', 0)) if audio_stream.get('sample_rate') else None,\n",
    "            'channels': audio_stream.get('channels'),\n",
    "            'bitrate': int(format_info.get('bit_rate', 0)) if format_info.get('bit_rate') else None,\n",
    "            'codec_name': audio_stream.get('codec_name'),\n",
    "            'duration_sec_actual': float(format_info.get('duration', 0)) if format_info.get('duration') else None,\n",
    "        }\n",
    "    \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return {\n",
    "            'sample_rate_hz': None,\n",
    "            'channels': None,\n",
    "            'bitrate': None,\n",
    "            'codec_name': None,\n",
    "            'duration_sec_actual': None,\n",
    "            'error': 'ffprobe timeout'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'sample_rate_hz': None,\n",
    "            'channels': None,\n",
    "            'bitrate': None,\n",
    "            'codec_name': None,\n",
    "            'duration_sec_actual': None,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def analyze_audio_quality(filepath):\n",
    "    \"\"\"\n",
    "    Analyze audio quality metrics using ffmpeg.\n",
    "    Returns dict with: peak_dbfs, silence_ratio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use ffmpeg volumedetect filter to get peak volume\n",
    "        result = subprocess.run([\n",
    "            'ffmpeg',\n",
    "            '-i', str(filepath),\n",
    "            '-af', 'volumedetect',\n",
    "            '-f', 'null',\n",
    "            '-'\n",
    "        ], capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        # Parse output for peak volume\n",
    "        peak_dbfs = None\n",
    "        for line in result.stderr.split('\\n'):\n",
    "            if 'max_volume:' in line:\n",
    "                try:\n",
    "                    # Extract value like \"max_volume: -23.5 dB\"\n",
    "                    peak_dbfs = float(line.split(':')[1].strip().split()[0])\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Use ffmpeg silencedetect filter to detect silence\n",
    "        result_silence = subprocess.run([\n",
    "            'ffmpeg',\n",
    "            '-i', str(filepath),\n",
    "            '-af', 'silencedetect=noise=-50dB:d=0.1',\n",
    "            '-f', 'null',\n",
    "            '-'\n",
    "        ], capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        # Parse silence detection output\n",
    "        silence_duration = 0.0\n",
    "        total_duration = 0.0\n",
    "        \n",
    "        for line in result_silence.stderr.split('\\n'):\n",
    "            if 'silence_duration:' in line:\n",
    "                try:\n",
    "                    duration = float(line.split('silence_duration:')[1].strip().split()[0])\n",
    "                    silence_duration += duration\n",
    "                except:\n",
    "                    pass\n",
    "            if 'Duration:' in line and total_duration == 0:\n",
    "                try:\n",
    "                    # Extract duration from \"Duration: 00:03:45.67\"\n",
    "                    time_str = line.split('Duration:')[1].strip().split(',')[0].strip()\n",
    "                    parts = time_str.split(':')\n",
    "                    if len(parts) == 3:\n",
    "                        hours, minutes, seconds = parts\n",
    "                        total_duration = float(hours) * 3600 + float(minutes) * 60 + float(seconds)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        silence_ratio = (silence_duration / total_duration) if total_duration > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'peak_dbfs': peak_dbfs,\n",
    "            'silence_ratio': silence_ratio\n",
    "        }\n",
    "    \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return {\n",
    "            'peak_dbfs': None,\n",
    "            'silence_ratio': None,\n",
    "            'error': 'quality analysis timeout'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'peak_dbfs': None,\n",
    "            'silence_ratio': None,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def compute_file_hash(filepath):\n",
    "    \"\"\"\n",
    "    Compute SHA256 hash of file for deduplication.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sha256_hash = hashlib.sha256()\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            # Read in chunks to handle large files\n",
    "            for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "                sha256_hash.update(chunk)\n",
    "        return sha256_hash.hexdigest()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "print(\"Loading track data...\")\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    # Load first track from checkpoint\n",
    "    with open(CHECKPOINT_FILE, 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        if first_line.strip():\n",
    "            test_track = json.loads(first_line)\n",
    "            print(f\"‚úì Loaded track from checkpoint file\")\n",
    "        else:\n",
    "            print(\"‚úó Checkpoint file is empty\")\n",
    "            test_track = None\n",
    "elif OUTPUT_FILE.exists():\n",
    "    # Load first track from final file\n",
    "    with open(OUTPUT_FILE, 'r') as f:\n",
    "        all_tracks = json.load(f)\n",
    "        if all_tracks:\n",
    "            test_track = all_tracks[0]\n",
    "            print(f\"‚úì Loaded track from final output file\")\n",
    "        else:\n",
    "            print(\"‚úó Output file has no tracks\")\n",
    "            test_track = None\n",
    "else:\n",
    "    print(\"‚úó No track data found. Run the fetch cell first.\")\n",
    "    test_track = None\n",
    "\n",
    "if test_track:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEST TRACK INFO\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Track ID: {test_track.get('id')}\")\n",
    "    print(f\"Name: {test_track.get('name')}\")\n",
    "    print(f\"Artist: {test_track.get('artist_name')}\")\n",
    "    print(f\"Album: {test_track.get('album_name')}\")\n",
    "    print(f\"Duration: {test_track.get('duration')}s ({test_track.get('duration')/60:.1f} minutes)\")\n",
    "    \n",
    "    # Get download URL from API\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FETCHING DOWNLOAD URL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    track_id = test_track.get('id')\n",
    "    \n",
    "    client = httpx.Client(timeout=30, follow_redirects=True)\n",
    "    try:\n",
    "        # Request track info with audiodownload format\n",
    "        response = client.get(f\"{JAMENDO_API_BASE}/tracks/\", params={\n",
    "            \"client_id\": JAMENDO_CLIENT_ID,\n",
    "            \"format\": \"json\",\n",
    "            \"id\": track_id,\n",
    "            \"audiodownload\": \"true\"\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        track_data = response.json()\n",
    "        \n",
    "        if track_data.get(\"results\"):\n",
    "            track_info = track_data[\"results\"][0]\n",
    "            download_url = track_info.get(\"audiodownload\")\n",
    "            \n",
    "            if download_url:\n",
    "                print(f\"‚úì Download URL obtained: {download_url}\")\n",
    "                \n",
    "                # Download the file\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"DOWNLOADING AUDIO FILE\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                # Generate filename\n",
    "                safe_name = \"\".join(c for c in test_track.get('name', 'track') if c.isalnum() or c in (' ', '-', '_')).strip()\n",
    "                safe_artist = \"\".join(c for c in test_track.get('artist_name', 'artist') if c.isalnum() or c in (' ', '-', '_')).strip()\n",
    "                filename = f\"{track_id}_{safe_artist}_{safe_name}.mp3\"\n",
    "                filepath = TEST_DOWNLOAD_DIR / filename\n",
    "                \n",
    "                print(f\"Downloading to: {filepath}\")\n",
    "                print(f\"Starting download...\")\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Stream download with progress\n",
    "                with client.stream(\"GET\", download_url) as r:\n",
    "                    r.raise_for_status()\n",
    "                    total_size = int(r.headers.get('content-length', 0))\n",
    "                    \n",
    "                    with open(filepath, 'wb') as f:\n",
    "                        downloaded = 0\n",
    "                        for chunk in r.iter_bytes(chunk_size=8192):\n",
    "                            f.write(chunk)\n",
    "                            downloaded += len(chunk)\n",
    "                            if total_size > 0:\n",
    "                                progress = (downloaded / total_size) * 100\n",
    "                                print(f\"\\rProgress: {downloaded:,} / {total_size:,} bytes ({progress:.1f}%)\", end='')\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                file_size_mb = filepath.stat().st_size / 1024 / 1024\n",
    "                \n",
    "                print(f\"\\n\\n‚úì Download complete!\")\n",
    "                print(f\"  File: {filepath.name}\")\n",
    "                print(f\"  Size: {file_size_mb:.2f} MB\")\n",
    "                print(f\"  Time: {elapsed:.1f} seconds\")\n",
    "                print(f\"  Speed: {file_size_mb/elapsed:.2f} MB/s\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # POST-DOWNLOAD METADATA EXTRACTION\n",
    "                # ============================================================\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"EXTRACTING AUDIO METADATA\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                # A) Technical metadata from ffprobe\n",
    "                print(\"Running ffprobe analysis...\")\n",
    "                tech_metadata = extract_audio_metadata_ffprobe(filepath)\n",
    "                \n",
    "                # B) Quality checks from ffmpeg\n",
    "                print(\"Running quality analysis...\")\n",
    "                quality_metadata = analyze_audio_quality(filepath)\n",
    "                \n",
    "                # C) SHA256 hash for deduplication\n",
    "                print(\"Computing file hash...\")\n",
    "                file_hash = compute_file_hash(filepath)\n",
    "                \n",
    "                # D) Segment metadata (not implemented, so None)\n",
    "                segment_metadata = {\n",
    "                    'segment_id': None,\n",
    "                    'segment_start_sec': None,\n",
    "                    'segment_end_sec': None\n",
    "                }\n",
    "                \n",
    "                # Compile complete metadata\n",
    "                complete_metadata = {\n",
    "                    # Original track info\n",
    "                    'track_id': track_id,\n",
    "                    'track_name': test_track.get('name'),\n",
    "                    'artist_name': test_track.get('artist_name'),\n",
    "                    'filename': filename,\n",
    "                    'file_path': str(filepath),\n",
    "                    'file_size_bytes': filepath.stat().st_size,\n",
    "                    \n",
    "                    # A) Technical metadata\n",
    "                    **tech_metadata,\n",
    "                    \n",
    "                    # B) Quality metrics\n",
    "                    **quality_metadata,\n",
    "                    \n",
    "                    # C) Deduplication\n",
    "                    'sha256': file_hash,\n",
    "                    \n",
    "                    # D) Segment metadata\n",
    "                    **segment_metadata\n",
    "                }\n",
    "                \n",
    "                # Print results\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"COMPLETE METADATA\")\n",
    "                print(\"=\"*60)\n",
    "                print(json.dumps(complete_metadata, indent=2))\n",
    "                \n",
    "                # Quality checks summary\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"QUALITY CHECKS SUMMARY\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                # Check for clipping (peak near 0 dBFS)\n",
    "                clipping_detected = False\n",
    "                if complete_metadata['peak_dbfs'] is not None:\n",
    "                    if complete_metadata['peak_dbfs'] > -1.0:\n",
    "                        clipping_detected = True\n",
    "                        print(f\"‚ö†Ô∏è  CLIPPING DETECTED: Peak = {complete_metadata['peak_dbfs']:.2f} dBFS\")\n",
    "                    else:\n",
    "                        print(f\"‚úì No clipping: Peak = {complete_metadata['peak_dbfs']:.2f} dBFS\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  Could not determine peak level\")\n",
    "                \n",
    "                # Check for high silence ratio\n",
    "                high_silence = False\n",
    "                if complete_metadata['silence_ratio'] is not None:\n",
    "                    if complete_metadata['silence_ratio'] > 0.20:\n",
    "                        high_silence = True\n",
    "                        print(f\"‚ö†Ô∏è  HIGH SILENCE RATIO: {complete_metadata['silence_ratio']:.1%} of audio is silent\")\n",
    "                    else:\n",
    "                        print(f\"‚úì Normal silence ratio: {complete_metadata['silence_ratio']:.1%}\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  Could not determine silence ratio\")\n",
    "                \n",
    "                # Summary counts (for single file)\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"SUMMARY COUNTS\")\n",
    "                print(\"=\"*60)\n",
    "                print(f\"Files analyzed: 1\")\n",
    "                print(f\"Files with clipping: {1 if clipping_detected else 0}\")\n",
    "                print(f\"Files with high silence (>20%): {1 if high_silence else 0}\")\n",
    "                \n",
    "                print(f\"\\n‚úì Test download and metadata extraction successful!\")\n",
    "                \n",
    "            else:\n",
    "                print(\"‚úó No download URL found in track data\")\n",
    "                print(f\"Available fields: {list(track_info.keys())}\")\n",
    "        else:\n",
    "            print(\"‚úó No track data returned from API\")\n",
    "            \n",
    "    except httpx.HTTPError as e:\n",
    "        print(f\"‚úó HTTP error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        client.close()\n",
    "else:\n",
    "    print(\"\\nSkipping test - no track data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q httpx pandas pyarrow tqdm tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type\n",
    ")\n",
    "from typing import Optional, Dict, List, Any\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_license(license_ccurl: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Parse Creative Commons license URL and return canonical form.\n",
    "    \n",
    "    Returns: 'cc-by', 'cc-by-sa', or None (for rejected licenses)\n",
    "    \n",
    "    Accepts:\n",
    "    - CC-BY 3.0/4.0: http://creativecommons.org/licenses/by/3.0/\n",
    "    - CC-BY-SA 3.0/4.0: http://creativecommons.org/licenses/by-sa/3.0/\n",
    "    \n",
    "    Rejects (returns None):\n",
    "    - CC-BY-NC (NonCommercial)\n",
    "    - CC-BY-ND (NoDerivatives)\n",
    "    - CC-BY-NC-SA\n",
    "    - CC-BY-NC-ND\n",
    "    - Any other license\n",
    "    \"\"\"\n",
    "    if not license_ccurl or not isinstance(license_ccurl, str):\n",
    "        return None\n",
    "    \n",
    "    # Normalize URL\n",
    "    url_lower = license_ccurl.lower().strip().rstrip('/')\n",
    "    \n",
    "    # Extract license type from URL pattern\n",
    "    # Expected: http(s)://creativecommons.org/licenses/{type}/{version}/\n",
    "    if 'creativecommons.org/licenses/' not in url_lower:\n",
    "        return None\n",
    "    \n",
    "    # Extract type component\n",
    "    parts = url_lower.split('creativecommons.org/licenses/')\n",
    "    if len(parts) != 2:\n",
    "        return None\n",
    "    \n",
    "    license_part = parts[1].split('/')[0]  # Get type before version\n",
    "    \n",
    "    # Strict allowlist matching\n",
    "    if license_part == 'by':\n",
    "        return 'cc-by'\n",
    "    elif license_part == 'by-sa':\n",
    "        return 'cc-by-sa'\n",
    "    else:\n",
    "        # Reject: by-nc, by-nd, by-nc-sa, by-nc-nd, etc.\n",
    "        return None\n",
    "\n",
    "# Unit tests\n",
    "assert canonicalize_license(\"http://creativecommons.org/licenses/by/3.0/\") == \"cc-by\"\n",
    "assert canonicalize_license(\"http://creativecommons.org/licenses/by-sa/4.0/\") == \"cc-by-sa\"\n",
    "assert canonicalize_license(\"http://creativecommons.org/licenses/by-nc/3.0/\") is None\n",
    "assert canonicalize_license(\"http://creativecommons.org/licenses/by-nc-sa/3.0/\") is None\n",
    "assert canonicalize_license(\"http://creativecommons.org/licenses/by-nd/3.0/\") is None\n",
    "assert canonicalize_license(None) is None\n",
    "assert canonicalize_license(\"\") is None\n",
    "print(\"‚úì License canonicalization tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JamendoAPIClient:\n",
    "    \"\"\"Jamendo API client with retry logic and rate limiting.\"\"\"\n",
    "    \n",
    "    def __init__(self, client_id: str, timeout: int = 30):\n",
    "        self.client_id = client_id\n",
    "        self.timeout = timeout\n",
    "        self.client = httpx.Client(timeout=timeout)\n",
    "        self.last_request_time = 0\n",
    "        self.min_request_interval = 0.1  # 100ms between requests\n",
    "    \n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Simple rate limiting: ensure minimum interval between requests.\"\"\"\n",
    "        elapsed = time.time() - self.last_request_time\n",
    "        if elapsed < self.min_request_interval:\n",
    "            time.sleep(self.min_request_interval - elapsed)\n",
    "        self.last_request_time = time.time()\n",
    "    \n",
    "    @retry(\n",
    "        stop=stop_after_attempt(RETRY_MAX_ATTEMPTS),\n",
    "        wait=wait_exponential(multiplier=RETRY_BACKOFF_FACTOR, min=1, max=60),\n",
    "        retry=retry_if_exception_type((httpx.TimeoutException, httpx.HTTPStatusError))\n",
    "    )\n",
    "    def fetch_tracks(\n",
    "        self, \n",
    "        offset: int = 0, \n",
    "        limit: int = 200,\n",
    "        include_fullcount: bool = False\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch tracks from Jamendo API with pagination.\n",
    "        \n",
    "        Returns: {\"headers\": {...}, \"results\": [...]}\n",
    "        \"\"\"\n",
    "        self._rate_limit()\n",
    "        \n",
    "        params = {\n",
    "            \"client_id\": self.client_id,\n",
    "            \"format\": \"json\",\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset,\n",
    "            \"audiodownload\": \"true\",  # Only downloadable tracks\n",
    "        }\n",
    "        \n",
    "        if include_fullcount:\n",
    "            params[\"fullcount\"] = \"true\"\n",
    "        \n",
    "        url = f\"{JAMENDO_API_BASE}/tracks/\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            if e.response.status_code == 429:\n",
    "                # Rate limit - tenacity will retry with backoff\n",
    "                print(f\"‚ö† Rate limited (429), will retry...\")\n",
    "                raise\n",
    "            elif e.response.status_code >= 500:\n",
    "                # Server error - tenacity will retry\n",
    "                print(f\"‚ö† Server error ({e.response.status_code}), will retry...\")\n",
    "                raise\n",
    "            else:\n",
    "                # Client error - don't retry\n",
    "                print(f\"‚úó Client error ({e.response.status_code}): {e}\")\n",
    "                raise RuntimeError(f\"API error: {e.response.status_code}\") from e\n",
    "    \n",
    "    def close(self):\n",
    "        self.client.close()\n",
    "\n",
    "# Test client initialization\n",
    "api_client = JamendoAPIClient(JAMENDO_CLIENT_ID, timeout=REQUEST_TIMEOUT)\n",
    "print(\"‚úì API client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state() -> Dict[str, Any]:\n",
    "    \"\"\"Load checkpoint state or return initial state.\"\"\"\n",
    "    if STATE_FILE.exists():\n",
    "        with open(STATE_FILE, 'r') as f:\n",
    "            state = json.load(f)\n",
    "        print(f\"‚úì Loaded checkpoint: offset={state['last_offset']}, fetched={state['total_fetched']}\")\n",
    "        return state\n",
    "    else:\n",
    "        return {\n",
    "            \"last_offset\": 0,\n",
    "            \"total_fetched\": 0,\n",
    "            \"total_passed_filter\": 0,\n",
    "            \"total_rejected\": 0,\n",
    "            \"total_catalog_size\": None,\n",
    "            \"start_time\": datetime.utcnow().isoformat(),\n",
    "            \"last_update_time\": None\n",
    "        }\n",
    "\n",
    "def save_state(state: Dict[str, Any]):\n",
    "    \"\"\"Save checkpoint state.\"\"\"\n",
    "    state[\"last_update_time\"] = datetime.utcnow().isoformat()\n",
    "    with open(STATE_FILE, 'w') as f:\n",
    "        json.dump(state, f, indent=2)\n",
    "\n",
    "# Load or initialize state\n",
    "state = load_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_track(track: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Filter and extract metadata from a track.\n",
    "    Returns None if track doesn't pass license filter.\n",
    "    \"\"\"\n",
    "    license_url = track.get(\"license_ccurl\")\n",
    "    canonical_license = canonicalize_license(license_url)\n",
    "    \n",
    "    if canonical_license is None:\n",
    "        return None\n",
    "    \n",
    "    # Extract relevant metadata\n",
    "    return {\n",
    "        \"id\": track.get(\"id\"),\n",
    "        \"name\": track.get(\"name\"),\n",
    "        \"duration\": track.get(\"duration\"),  # in seconds\n",
    "        \"artist_id\": track.get(\"artist_id\"),\n",
    "        \"artist_name\": track.get(\"artist_name\"),\n",
    "        \"album_id\": track.get(\"album_id\"),\n",
    "        \"album_name\": track.get(\"album_name\"),\n",
    "        \"license\": canonical_license,\n",
    "        \"license_ccurl\": license_url,\n",
    "        \"releasedate\": track.get(\"releasedate\"),\n",
    "        \"audiodownload_allowed\": track.get(\"audiodownload_allowed\"),\n",
    "    }\n",
    "\n",
    "# Main crawl loop\n",
    "offset = state[\"last_offset\"]\n",
    "page_count = 0\n",
    "audit_passed = []\n",
    "audit_rejected = []\n",
    "\n",
    "# Open JSONL in append mode\n",
    "jsonl_mode = 'a' if offset > 0 else 'w'\n",
    "jsonl_file = open(JSONL_FILE, jsonl_mode)\n",
    "\n",
    "try:\n",
    "    # Get total catalog size on first request\n",
    "    if state[\"total_catalog_size\"] is None:\n",
    "        print(\"Fetching catalog size...\")\n",
    "        first_response = api_client.fetch_tracks(offset=0, limit=1, include_fullcount=True)\n",
    "        total_size = first_response[\"headers\"].get(\"results_fullcount\", 0)\n",
    "        state[\"total_catalog_size\"] = total_size\n",
    "        print(f\"Total catalog size: {total_size:,} tracks\")\n",
    "    \n",
    "    total_size = state[\"total_catalog_size\"]\n",
    "    total_pages = (total_size // PAGE_SIZE) + (1 if total_size % PAGE_SIZE else 0)\n",
    "    \n",
    "    if MAX_PAGES > 0:\n",
    "        total_pages = min(total_pages, MAX_PAGES)\n",
    "        print(f\"DRY RUN: Processing {total_pages} pages only\")\n",
    "    \n",
    "    # Progress bar\n",
    "    with tqdm(total=total_pages, initial=offset // PAGE_SIZE, desc=\"Crawling pages\", unit=\"page\") as pbar:\n",
    "        while True:\n",
    "            # Check if we've reached max pages (dry run mode)\n",
    "            if MAX_PAGES > 0 and page_count >= MAX_PAGES:\n",
    "                print(f\"Reached MAX_PAGES limit ({MAX_PAGES})\")\n",
    "                break\n",
    "            \n",
    "            # Check if we've exhausted the catalog\n",
    "            if offset >= total_size:\n",
    "                print(f\"Completed: processed all {total_size:,} tracks\")\n",
    "                break\n",
    "            \n",
    "            # Fetch page\n",
    "            response = api_client.fetch_tracks(offset=offset, limit=PAGE_SIZE)\n",
    "            results = response.get(\"results\", [])\n",
    "            \n",
    "            if not results:\n",
    "                print(f\"No more results at offset {offset}\")\n",
    "                break\n",
    "            \n",
    "            # Process tracks\n",
    "            for track in results:\n",
    "                state[\"total_fetched\"] += 1\n",
    "                \n",
    "                processed = process_track(track)\n",
    "                if processed:\n",
    "                    # Write to JSONL immediately (streaming)\n",
    "                    jsonl_file.write(json.dumps(processed) + '\\n')\n",
    "                    state[\"total_passed_filter\"] += 1\n",
    "                    \n",
    "                    # Collect audit samples\n",
    "                    if len(audit_passed) < 10:\n",
    "                        audit_passed.append({\n",
    "                            \"id\": track.get(\"id\"),\n",
    "                            \"name\": track.get(\"name\"),\n",
    "                            \"license_ccurl\": track.get(\"license_ccurl\"),\n",
    "                            \"canonical\": processed[\"license\"]\n",
    "                        })\n",
    "                else:\n",
    "                    state[\"total_rejected\"] += 1\n",
    "                    \n",
    "                    # Collect audit samples\n",
    "                    if len(audit_rejected) < 10:\n",
    "                        audit_rejected.append({\n",
    "                            \"id\": track.get(\"id\"),\n",
    "                            \"name\": track.get(\"name\"),\n",
    "                            \"license_ccurl\": track.get(\"license_ccurl\"),\n",
    "                            \"reason\": \"license_not_allowed\"\n",
    "                        })\n",
    "            \n",
    "            # Update state\n",
    "            offset += len(results)\n",
    "            state[\"last_offset\"] = offset\n",
    "            page_count += 1\n",
    "            \n",
    "            # Checkpoint periodically\n",
    "            if page_count % CHECKPOINT_INTERVAL == 0:\n",
    "                jsonl_file.flush()\n",
    "                save_state(state)\n",
    "            \n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\n",
    "                \"passed\": state[\"total_passed_filter\"],\n",
    "                \"rejected\": state[\"total_rejected\"]\n",
    "            })\n",
    "    \n",
    "    # Final checkpoint\n",
    "    jsonl_file.flush()\n",
    "    save_state(state)\n",
    "    \n",
    "finally:\n",
    "    jsonl_file.close()\n",
    "    api_client.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CRAWL COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total tracks fetched: {state['total_fetched']:,}\")\n",
    "print(f\"Passed filter (CC-BY/CC-BY-SA): {state['total_passed_filter']:,}\")\n",
    "print(f\"Rejected (NC/ND/other): {state['total_rejected']:,}\")\n",
    "print(f\"Pass rate: {100 * state['total_passed_filter'] / state['total_fetched']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AUDIT TRAIL - Sample Tracks\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n‚úì PASSED FILTER (CC-BY/CC-BY-SA):\")\n",
    "for i, track in enumerate(audit_passed, 1):\n",
    "    print(f\"{i}. [{track['id']}] {track['name']}\")\n",
    "    print(f\"   License URL: {track['license_ccurl']}\")\n",
    "    print(f\"   Canonical: {track['canonical']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n‚úó REJECTED (NC/ND/other):\")\n",
    "for i, track in enumerate(audit_rejected, 1):\n",
    "    print(f\"{i}. [{track['id']}] {track['name']}\")\n",
    "    print(f\"   License URL: {track['license_ccurl']}\")\n",
    "    print(f\"   Reason: {track['reason']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSONL into DataFrame\n",
    "print(\"Loading metadata for aggregation...\")\n",
    "df = pd.read_json(JSONL_FILE, lines=True)\n",
    "print(f\"Loaded {len(df):,} tracks\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "total_tracks = len(df)\n",
    "total_duration_seconds = df['duration'].sum()\n",
    "total_hours = total_duration_seconds / 3600\n",
    "\n",
    "summary = {\n",
    "    \"generated_at\": datetime.utcnow().isoformat(),\n",
    "    \"source\": \"jamendo_api\",\n",
    "    \"license_filter\": list(ALLOWED_LICENSES),\n",
    "    \"total_tracks\": int(total_tracks),\n",
    "    \"total_duration_seconds\": float(total_duration_seconds),\n",
    "    \"total_duration_hours\": float(total_hours),\n",
    "    \"total_catalog_tracks_fetched\": state[\"total_fetched\"],\n",
    "    \"pass_rate_percent\": float(100 * state[\"total_passed_filter\"] / state['total_fetched']),\n",
    "    \"crawl_start_time\": state[\"start_time\"],\n",
    "    \"crawl_end_time\": state[\"last_update_time\"],\n",
    "}\n",
    "\n",
    "# Save summary JSON\n",
    "with open(SUMMARY_FILE, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total tracks: {summary['total_tracks']:,}\")\n",
    "print(f\"Total duration: {summary['total_duration_hours']:,.1f} hours\")\n",
    "print(f\"Average track duration: {total_duration_seconds / total_tracks / 60:.1f} minutes\")\n",
    "print(f\"Pass rate: {summary['pass_rate_percent']:.1f}%\")\n",
    "print(f\"\\n‚úì Saved: {SUMMARY_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by license type\n",
    "license_stats = df.groupby('license').agg({\n",
    "    'duration': ['count', 'sum']\n",
    "}).reset_index()\n",
    "\n",
    "license_stats.columns = ['license', 'track_count', 'total_duration_seconds']\n",
    "license_stats['total_duration_hours'] = license_stats['total_duration_seconds'] / 3600\n",
    "license_stats['percentage_of_tracks'] = 100 * license_stats['track_count'] / total_tracks\n",
    "\n",
    "# Save CSV\n",
    "license_stats.to_csv(LICENSE_CSV, index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BREAKDOWN BY LICENSE TYPE\")\n",
    "print(\"=\"*60)\n",
    "print(license_stats.to_string(index=False))\n",
    "print(f\"\\n‚úì Saved: {LICENSE_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute duration percentiles\n",
    "duration_minutes = df['duration'] / 60\n",
    "\n",
    "duration_stats = pd.DataFrame({\n",
    "    'metric': ['mean', 'median (p50)', 'p90', 'p95', 'p99', 'min', 'max'],\n",
    "    'duration_minutes': [\n",
    "        duration_minutes.mean(),\n",
    "        duration_minutes.quantile(0.50),\n",
    "        duration_minutes.quantile(0.90),\n",
    "        duration_minutes.quantile(0.95),\n",
    "        duration_minutes.quantile(0.99),\n",
    "        duration_minutes.min(),\n",
    "        duration_minutes.max(),\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Save CSV\n",
    "duration_stats.to_csv(DURATION_CSV, index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DURATION STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(duration_stats.to_string(index=False))\n",
    "print(f\"\\n‚úì Saved: {DURATION_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Parquet (more efficient for large datasets)\n",
    "try:\n",
    "    df.to_parquet(PARQUET_FILE, index=False, compression='snappy')\n",
    "    print(f\"‚úì Saved Parquet: {PARQUET_FILE}\")\n",
    "    print(f\"  JSONL size: {JSONL_FILE.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "    print(f\"  Parquet size: {PARQUET_FILE.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Failed to save Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL OUTPUTS GENERATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  1. {JSONL_FILE.name} - Raw filtered metadata (append-only)\")\n",
    "print(f\"  2. {SUMMARY_FILE.name} - Topline metrics\")\n",
    "print(f\"  3. {LICENSE_CSV.name} - License breakdown\")\n",
    "print(f\"  4. {DURATION_CSV.name} - Duration statistics\")\n",
    "print(f\"  5. {STATE_FILE.name} - Checkpoint state (for resume)\")\n",
    "if PARQUET_FILE.exists():\n",
    "    print(f\"  6. {PARQUET_FILE.name} - Parquet archive (optional)\")\n",
    "\n",
    "print(f\"\\n‚úì Pipeline complete!\")\n",
    "print(f\"\\nTo resume crawl if interrupted:\")\n",
    "print(f\"  - Re-run this notebook (it will resume from offset {state['last_offset']})\")\n",
    "print(f\"\\nTo start fresh:\")\n",
    "print(f\"  - Delete {STATE_FILE} and {JSONL_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
