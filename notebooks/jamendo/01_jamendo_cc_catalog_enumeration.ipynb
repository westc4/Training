{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 â€” Jamendo CC Catalog Enumeration\n",
    "\n",
    "Enumerate Jamendo music catalog via API and compute metadata statistics for CC-BY and CC-BY-SA licensed tracks only.\n",
    "\n",
    "Outputs: summary JSON, CSV breakdowns, optional Parquet archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Base paths\n",
    "BASE_DIR = Path(\"/root/workspace\")\n",
    "OUTPUT_DIR = BASE_DIR / \"data\" / \"jamendo_cc_catalog\"\n",
    "#OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "JAMENDO_CLIENT_ID=\"48ecf016\"\n",
    "# API Configuration\n",
    "\n",
    "if not JAMENDO_CLIENT_ID:\n",
    "    raise RuntimeError(\"Set JAMENDO_CLIENT_ID environment variable\")\n",
    "\n",
    "JAMENDO_API_BASE = \"https://api.jamendo.com/v3.0\"\n",
    "\n",
    "# Processing Configuration\n",
    "MAX_PAGES = int(os.environ.get(\"MAX_PAGES\", \"0\"))  # 0 = all, 5 = dry run\n",
    "PAGE_SIZE = 200  # Max allowed by Jamendo API\n",
    "CHECKPOINT_INTERVAL = 10  # Save state every N pages\n",
    "REQUEST_TIMEOUT = 30\n",
    "RETRY_MAX_ATTEMPTS = 5\n",
    "RETRY_BACKOFF_FACTOR = 2\n",
    "\n",
    "# License Allowlist (strict)\n",
    "ALLOWED_LICENSES = {\"cc-by\", \"cc-by-sa\"}\n",
    "\n",
    "# Output files\n",
    "STATE_FILE = OUTPUT_DIR / \"state.json\"\n",
    "JSONL_FILE = OUTPUT_DIR / \"jamendo_cc_tracks_metadata.jsonl\"\n",
    "SUMMARY_FILE = OUTPUT_DIR / \"jamendo_cc_hours_summary.json\"\n",
    "LICENSE_CSV = OUTPUT_DIR / \"jamendo_cc_hours_by_license.csv\"\n",
    "DURATION_CSV = OUTPUT_DIR / \"jamendo_cc_duration_stats.csv\"\n",
    "PARQUET_FILE = OUTPUT_DIR / \"jamendo_cc_tracks_metadata.parquet\"\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Client ID configured: {'âœ“' if JAMENDO_CLIENT_ID else 'âœ—'}\")\n",
    "print(f\"Mode: {'DRY RUN (max ' + str(MAX_PAGES) + ' pages)' if MAX_PAGES > 0 else 'FULL CRAWL'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found checkpoint: resuming from offset 13,800 (13,800 tracks)\n",
      "Resuming fetch: 13,800 tracks already fetched\n",
      "Total catalog size: 848,767 tracks\n",
      "Total pages to fetch: 4,244\n",
      "Rate limit: 0.1s delay between requests\n",
      "Checkpoint: saving every 50 pages\n",
      "API fields: licenses + musicinfo (tags, instruments, vartist, etc.)\n",
      "\n",
      "ðŸ“Š Resume Status:\n",
      "   Already fetched: 13,800 tracks (1.6%)\n",
      "   Remaining: 834,967 tracks\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462e1b82d98c49489233d3c8dd037e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching tracks:   2%|1         | 13800/848767 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "âš ï¸  INTERRUPTED - Cleaning up resources...\n",
      "âœ“ Checkpoint saved at offset 31,200\n",
      "âœ“ You can resume by re-running this cell\n",
      "âœ“ Checkpoint file closed\n",
      "âœ“ HTTP client closed\n",
      "\n",
      "ðŸ›‘ Fetch interrupted. Progress has been saved.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
     ]
    }
   ],
   "source": [
    "# Test: Fetch FULL CATALOG with robust CC license filtering and diagnostics\n",
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm  # Auto-detects environment (notebook widgets or text-based)\n",
    "import time\n",
    "import sys\n",
    "\n",
    "JAMENDO_CLIENT_ID = \"48ecf016\"\n",
    "JAMENDO_API_BASE = \"https://api.jamendo.com/v3.0\"\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_FILE = Path(\"/Users/cliftonwest/Documents/GitHub/Training/notebooks/jamendo/full_track_info.json\")\n",
    "STATE_FILE = Path(\"/Users/cliftonwest/Documents/GitHub/Training/notebooks/jamendo/fetch_state.json\")\n",
    "CHECKPOINT_FILE = Path(\"/Users/cliftonwest/Documents/GitHub/Training/notebooks/jamendo/tracks_checkpoint.jsonl\")\n",
    "\n",
    "# Rate limiting settings\n",
    "REQUEST_DELAY = 0.1  # Delay between requests in seconds (100ms)\n",
    "MAX_RETRIES = 5\n",
    "RETRY_DELAY = 2  # Initial retry delay in seconds\n",
    "CHECKPOINT_INTERVAL = 50  # Save checkpoint every N pages\n",
    "\n",
    "def extract_canonical_license(track):\n",
    "    \"\"\"\n",
    "    Robust license extractor - checks multiple fields and nested structures.\n",
    "    Returns: 'cc-by', 'cc-by-sa', or None (rejected)\n",
    "    \"\"\"\n",
    "    # Try multiple license fields\n",
    "    license_url = None\n",
    "    \n",
    "    # Check license_ccurl first\n",
    "    if track.get('license_ccurl'):\n",
    "        license_url = track.get('license_ccurl')\n",
    "    # Check licensecurl (alternate spelling)\n",
    "    elif track.get('licensecurl'):\n",
    "        license_url = track.get('licensecurl')\n",
    "    # Check nested licenses array\n",
    "    elif track.get('licenses') and isinstance(track.get('licenses'), list) and len(track.get('licenses')) > 0:\n",
    "        first_license = track['licenses'][0]\n",
    "        if isinstance(first_license, dict):\n",
    "            license_url = first_license.get('url') or first_license.get('ccurl')\n",
    "        elif isinstance(first_license, str):\n",
    "            license_url = first_license\n",
    "    \n",
    "    if not license_url or not isinstance(license_url, str):\n",
    "        return None\n",
    "    \n",
    "    # Normalize URL\n",
    "    url_lower = license_url.lower().strip().rstrip('/')\n",
    "    \n",
    "    # Check for creativecommons.org pattern\n",
    "    if 'creativecommons.org/licenses/' not in url_lower:\n",
    "        return None\n",
    "    \n",
    "    # Extract license type\n",
    "    parts = url_lower.split('creativecommons.org/licenses/')\n",
    "    if len(parts) != 2:\n",
    "        return None\n",
    "    \n",
    "    license_part = parts[1].split('/')[0]\n",
    "    \n",
    "    # Reject NC (NonCommercial) or ND (NoDerivatives)\n",
    "    if 'nc' in license_part or 'nd' in license_part:\n",
    "        return None\n",
    "    \n",
    "    # Accept only CC-BY or CC-BY-SA\n",
    "    if license_part == 'by':\n",
    "        return 'cc-by'\n",
    "    elif license_part == 'by-sa':\n",
    "        return 'cc-by-sa'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_license_flags(track):\n",
    "    \"\"\"\n",
    "    Extract cc, ccnc, ccnd flags from track.\n",
    "    Checks both top-level and nested licenses object.\n",
    "    Returns tuple: (cc, ccnc, ccnd)\n",
    "    \"\"\"\n",
    "    # Try top-level first\n",
    "    cc_val = track.get('cc')\n",
    "    ccnc_val = track.get('ccnc')\n",
    "    ccnd_val = track.get('ccnd')\n",
    "    \n",
    "    # If not found, check inside licenses object\n",
    "    licenses_obj = track.get('licenses')\n",
    "    if licenses_obj and isinstance(licenses_obj, dict):\n",
    "        if cc_val is None:\n",
    "            cc_val = licenses_obj.get('cc')\n",
    "        if ccnc_val is None:\n",
    "            ccnc_val = licenses_obj.get('ccnc')\n",
    "        if ccnd_val is None:\n",
    "            ccnd_val = licenses_obj.get('ccnd')\n",
    "    \n",
    "    # Convert string \"true\"/\"false\" to boolean\n",
    "    if isinstance(cc_val, str):\n",
    "        cc_val = cc_val.lower() == 'true'\n",
    "    if isinstance(ccnc_val, str):\n",
    "        ccnc_val = ccnc_val.lower() == 'true'\n",
    "    if isinstance(ccnd_val, str):\n",
    "        ccnd_val = ccnd_val.lower() == 'true'\n",
    "    \n",
    "    return cc_val, ccnc_val, ccnd_val\n",
    "\n",
    "def fetch_with_retry(client, url, params, max_retries=MAX_RETRIES):\n",
    "    \"\"\"Fetch with exponential backoff retry on rate limit errors.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            if e.response.status_code == 429:\n",
    "                # Rate limited - wait and retry with exponential backoff\n",
    "                wait_time = RETRY_DELAY * (2 ** attempt)\n",
    "                print(f\"\\nâš  Rate limited (429). Waiting {wait_time}s before retry {attempt + 1}/{max_retries}...\")\n",
    "                time.sleep(wait_time)\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "        except httpx.TimeoutException:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            wait_time = RETRY_DELAY * (2 ** attempt)\n",
    "            print(f\"\\nâš  Timeout. Waiting {wait_time}s before retry {attempt + 1}/{max_retries}...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    raise RuntimeError(f\"Failed after {max_retries} retries\")\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load checkpoint state if exists.\"\"\"\n",
    "    if STATE_FILE.exists():\n",
    "        with open(STATE_FILE, 'r') as f:\n",
    "            state = json.load(f)\n",
    "        print(f\"âœ“ Found checkpoint: resuming from offset {state['last_offset']:,} ({state['tracks_fetched']:,} tracks)\")\n",
    "        return state\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(state):\n",
    "    \"\"\"Save checkpoint state.\"\"\"\n",
    "    with open(STATE_FILE, 'w') as f:\n",
    "        json.dump(state, f, indent=2)\n",
    "\n",
    "def load_existing_tracks():\n",
    "    \"\"\"Load tracks from checkpoint JSONL file.\"\"\"\n",
    "    if not CHECKPOINT_FILE.exists():\n",
    "        return []\n",
    "    \n",
    "    tracks = []\n",
    "    with open(CHECKPOINT_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                tracks.append(json.loads(line))\n",
    "    return tracks\n",
    "\n",
    "# Create client\n",
    "client = httpx.Client(timeout=30)\n",
    "checkpoint_file = None\n",
    "PAGE_SIZE = 200  # Max allowed by Jamendo API\n",
    "\n",
    "try:\n",
    "    # Check for existing checkpoint\n",
    "    checkpoint = load_checkpoint()\n",
    "    \n",
    "    if checkpoint:\n",
    "        offset = checkpoint['last_offset']\n",
    "        all_tracks = load_existing_tracks()\n",
    "        total_catalog_size = checkpoint['total_catalog_size']\n",
    "        tracks_already_fetched = len(all_tracks)\n",
    "        print(f\"Resuming fetch: {tracks_already_fetched:,} tracks already fetched\")\n",
    "    else:\n",
    "        # Get total catalog size first\n",
    "        print(\"Starting fresh fetch...\")\n",
    "        print(\"Fetching catalog size...\")\n",
    "        first_data = fetch_with_retry(client, f\"{JAMENDO_API_BASE}/tracks/\", {\n",
    "            \"client_id\": JAMENDO_CLIENT_ID,\n",
    "            \"format\": \"json\",\n",
    "            \"limit\": 1,\n",
    "            \"offset\": 0,\n",
    "            \"audiodownload\": \"true\",\n",
    "            \"include\": \"licenses+musicinfo\",  # Include licenses and music metadata\n",
    "            \"fullcount\": \"true\"\n",
    "        })\n",
    "        \n",
    "        total_catalog_size = first_data.get(\"headers\", {}).get(\"results_fullcount\", 0)\n",
    "        offset = 0\n",
    "        all_tracks = []\n",
    "        tracks_already_fetched = 0\n",
    "        \n",
    "        # Initialize checkpoint state\n",
    "        checkpoint = {\n",
    "            'last_offset': 0,\n",
    "            'tracks_fetched': 0,\n",
    "            'total_catalog_size': total_catalog_size\n",
    "        }\n",
    "        save_checkpoint(checkpoint)\n",
    "        \n",
    "        # Create empty checkpoint file\n",
    "        CHECKPOINT_FILE.write_text('')\n",
    "    \n",
    "    print(f\"Total catalog size: {total_catalog_size:,} tracks\")\n",
    "    total_pages = (total_catalog_size // PAGE_SIZE) + (1 if total_catalog_size % PAGE_SIZE else 0)\n",
    "    print(f\"Total pages to fetch: {total_pages:,}\")\n",
    "    print(f\"Rate limit: {REQUEST_DELAY}s delay between requests\")\n",
    "    print(f\"Checkpoint: saving every {CHECKPOINT_INTERVAL} pages\")\n",
    "    print(f\"API fields: licenses + musicinfo (tags, instruments, vartist, etc.)\")\n",
    "    \n",
    "    # Show resume progress\n",
    "    remaining_tracks = total_catalog_size - tracks_already_fetched\n",
    "    if tracks_already_fetched > 0:\n",
    "        progress_pct = 100 * tracks_already_fetched / total_catalog_size\n",
    "        print(f\"\\nðŸ“Š Resume Status:\")\n",
    "        print(f\"   Already fetched: {tracks_already_fetched:,} tracks ({progress_pct:.1f}%)\")\n",
    "        print(f\"   Remaining: {remaining_tracks:,} tracks\")\n",
    "    print()\n",
    "    \n",
    "    # Open checkpoint file in append mode\n",
    "    checkpoint_file = open(CHECKPOINT_FILE, 'a')\n",
    "    page_count = 0\n",
    "    \n",
    "    # Fetch all tracks with progress bar\n",
    "    # Use tqdm.auto for automatic environment detection (works with or without ipywidgets)\n",
    "    with tqdm(total=total_catalog_size, \n",
    "              initial=tracks_already_fetched, \n",
    "              desc=\"Fetching tracks\", \n",
    "              unit=\"track\",\n",
    "              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:\n",
    "        \n",
    "        while offset < total_catalog_size:\n",
    "            params = {\n",
    "                \"client_id\": JAMENDO_CLIENT_ID,\n",
    "                \"format\": \"json\",\n",
    "                \"limit\": PAGE_SIZE,\n",
    "                \"offset\": offset,\n",
    "                \"audiodownload\": \"true\",\n",
    "                \"include\": \"licenses+musicinfo\",  # Include licenses and music metadata\n",
    "            }\n",
    "            \n",
    "            # Fetch with retry logic\n",
    "            data = fetch_with_retry(client, f\"{JAMENDO_API_BASE}/tracks/\", params)\n",
    "            \n",
    "            tracks = data.get(\"results\", [])\n",
    "            if not tracks:\n",
    "                print(f\"\\nNo more tracks at offset {offset}\")\n",
    "                break\n",
    "            \n",
    "            # Append tracks to list and write to checkpoint file\n",
    "            all_tracks.extend(tracks)\n",
    "            for track in tracks:\n",
    "                checkpoint_file.write(json.dumps(track) + '\\n')\n",
    "            \n",
    "            offset += len(tracks)\n",
    "            page_count += 1\n",
    "            pbar.update(len(tracks))\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if page_count % CHECKPOINT_INTERVAL == 0:\n",
    "                checkpoint_file.flush()\n",
    "                checkpoint['last_offset'] = offset\n",
    "                checkpoint['tracks_fetched'] = len(all_tracks)\n",
    "                save_checkpoint(checkpoint)\n",
    "            \n",
    "            # Rate limiting: wait between requests\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "    \n",
    "    # Close checkpoint file\n",
    "    checkpoint_file.close()\n",
    "    checkpoint_file = None\n",
    "    \n",
    "    print(f\"\\nTotal tracks fetched: {len(all_tracks):,}\")\n",
    "    \n",
    "    # Save final JSON file\n",
    "    print(f\"Saving final file to {OUTPUT_FILE}...\")\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        json.dump(all_tracks, f, indent=2)\n",
    "    print(f\"âœ“ Saved full track info to: {OUTPUT_FILE}\")\n",
    "    print(f\"  File size: {OUTPUT_FILE.stat().st_size / 1024 / 1024:.1f} MB\\n\")\n",
    "    \n",
    "    # Clean up checkpoint files\n",
    "    print(\"Cleaning up checkpoint files...\")\n",
    "    if STATE_FILE.exists():\n",
    "        STATE_FILE.unlink()\n",
    "    if CHECKPOINT_FILE.exists():\n",
    "        CHECKPOINT_FILE.unlink()\n",
    "    print(\"âœ“ Checkpoint files removed\\n\")\n",
    "    \n",
    "    # Analytics counters\n",
    "    total_tracks = len(all_tracks)\n",
    "    empty_license_count = 0\n",
    "    content_id_restricted_count = 0\n",
    "    cc_flags_condition_count = 0\n",
    "    no_nc_nd_count = 0\n",
    "    cc_flags_passed_examples = []\n",
    "    no_nc_nd_examples = []\n",
    "    passed_cc_filter = []\n",
    "    rejected_examples = []\n",
    "    \n",
    "    print(\"Analyzing licenses...\")\n",
    "    \n",
    "    # Process tracks with progress bar\n",
    "    for track in tqdm(all_tracks, desc=\"Analyzing licenses\", unit=\"track\"):\n",
    "        # Get CC flags from track (handles nested licenses object)\n",
    "        cc_val, ccnc_val, ccnd_val = get_license_flags(track)\n",
    "        \n",
    "        # Check: cc==true && ccnc==false && ccnd==false\n",
    "        if cc_val == True and ccnc_val == False and ccnd_val == False:\n",
    "            cc_flags_condition_count += 1\n",
    "            if len(cc_flags_passed_examples) < 5:\n",
    "                licenses_obj = track.get('licenses', {})\n",
    "                cc_flags_passed_examples.append({\n",
    "                    'id': track.get('id'),\n",
    "                    'name': track.get('name'),\n",
    "                    'cc': licenses_obj.get('cc') if isinstance(licenses_obj, dict) else track.get('cc'),\n",
    "                    'ccnc': licenses_obj.get('ccnc') if isinstance(licenses_obj, dict) else track.get('ccnc'),\n",
    "                    'ccnd': licenses_obj.get('ccnd') if isinstance(licenses_obj, dict) else track.get('ccnd'),\n",
    "                    'license_url': track.get('license_ccurl') or track.get('licensecurl') or '(empty)'\n",
    "                })\n",
    "        \n",
    "        # Check: ccnc==false && ccnd==false (regardless of cc)\n",
    "        if ccnc_val == False and ccnd_val == False:\n",
    "            no_nc_nd_count += 1\n",
    "            if len(no_nc_nd_examples) < 5:\n",
    "                licenses_obj = track.get('licenses', {})\n",
    "                no_nc_nd_examples.append({\n",
    "                    'id': track.get('id'),\n",
    "                    'name': track.get('name'),\n",
    "                    'cc': licenses_obj.get('cc') if isinstance(licenses_obj, dict) else track.get('cc'),\n",
    "                    'ccnc': licenses_obj.get('ccnc') if isinstance(licenses_obj, dict) else track.get('ccnc'),\n",
    "                    'ccnd': licenses_obj.get('ccnd') if isinstance(licenses_obj, dict) else track.get('ccnd'),\n",
    "                    'license_url': track.get('license_ccurl') or track.get('licensecurl') or '(empty)'\n",
    "                })\n",
    "        \n",
    "        # Check if license URL is empty\n",
    "        license_url = track.get('license_ccurl') or track.get('licensecurl') or ''\n",
    "        if not license_url:\n",
    "            empty_license_count += 1\n",
    "            if len(rejected_examples) < 5:\n",
    "                rejected_examples.append({\n",
    "                    'id': track.get('id'),\n",
    "                    'name': track.get('name'),\n",
    "                    'content_id_free': track.get('content_id_free'),\n",
    "                    'license_url': '(empty)',\n",
    "                    'reason': 'empty_license'\n",
    "                })\n",
    "            continue\n",
    "        \n",
    "        # Check content_id_free\n",
    "        if track.get('content_id_free') == False:\n",
    "            content_id_restricted_count += 1\n",
    "            if len(rejected_examples) < 5:\n",
    "                rejected_examples.append({\n",
    "                    'id': track.get('id'),\n",
    "                    'name': track.get('name'),\n",
    "                    'content_id_free': track.get('content_id_free'),\n",
    "                    'license_url': license_url,\n",
    "                    'reason': 'content_id_restricted'\n",
    "                })\n",
    "            continue\n",
    "        \n",
    "        # Try to extract canonical license\n",
    "        canonical = extract_canonical_license(track)\n",
    "        \n",
    "        if canonical:\n",
    "            passed_cc_filter.append({\n",
    "                'id': track.get('id'),\n",
    "                'name': track.get('name'),\n",
    "                'content_id_free': track.get('content_id_free'),\n",
    "                'license_url': license_url,\n",
    "                'canonical': canonical\n",
    "            })\n",
    "        else:\n",
    "            if len(rejected_examples) < 5:\n",
    "                rejected_examples.append({\n",
    "                    'id': track.get('id'),\n",
    "                    'name': track.get('name'),\n",
    "                    'content_id_free': track.get('content_id_free'),\n",
    "                    'license_url': license_url,\n",
    "                    'reason': 'license_not_cc_by_or_cc_by_sa'\n",
    "                })\n",
    "    \n",
    "    # Print diagnostics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"JAMENDO FULL CATALOG LICENSE DIAGNOSTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total tracks in catalog: {total_tracks:,}\")\n",
    "    print(f\"Tracks with cc==true && ccnc==false && ccnd==false: {cc_flags_condition_count:,} ({100*cc_flags_condition_count/total_tracks:.1f}%)\")\n",
    "    print(f\"Tracks with ccnc==false && ccnd==false (any cc): {no_nc_nd_count:,} ({100*no_nc_nd_count/total_tracks:.1f}%)\")\n",
    "    print(f\"Tracks with empty license URL: {empty_license_count:,} ({100*empty_license_count/total_tracks:.1f}%)\")\n",
    "    print(f\"Tracks with content_id_free=false: {content_id_restricted_count:,} ({100*content_id_restricted_count/total_tracks:.1f}%)\")\n",
    "    print(f\"Tracks passing CC-BY/CC-BY-SA filter: {len(passed_cc_filter):,} ({100*len(passed_cc_filter)/total_tracks:.1f}%)\")\n",
    "    \n",
    "    # Show CC flags examples\n",
    "    if cc_flags_passed_examples:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ðŸ” CC FLAGS CONDITION EXAMPLES (first 5)\")\n",
    "        print(\"   Tracks where cc==true && ccnc==false && ccnd==false\")\n",
    "        print(\"=\"*60)\n",
    "        for track in cc_flags_passed_examples:\n",
    "            print(f\"ID: {track['id']}\")\n",
    "            print(f\"  Name: {track['name']}\")\n",
    "            print(f\"  cc: {track['cc']}\")\n",
    "            print(f\"  ccnc: {track['ccnc']}\")\n",
    "            print(f\"  ccnd: {track['ccnd']}\")\n",
    "            print(f\"  License URL: {track['license_url']}\")\n",
    "            print()\n",
    "    \n",
    "    # Show no NC/ND examples\n",
    "    if no_nc_nd_examples:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ðŸ” NO NC/ND CONDITION EXAMPLES (first 5)\")\n",
    "        print(\"   Tracks where ccnc==false && ccnd==false (any cc)\")\n",
    "        print(\"=\"*60)\n",
    "        for track in no_nc_nd_examples:\n",
    "            print(f\"ID: {track['id']}\")\n",
    "            print(f\"  Name: {track['name']}\")\n",
    "            print(f\"  cc: {track['cc']}\")\n",
    "            print(f\"  ccnc: {track['ccnc']}\")\n",
    "            print(f\"  ccnd: {track['ccnd']}\")\n",
    "            print(f\"  License URL: {track['license_url']}\")\n",
    "            print()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… PASSED FILTER - CC-BY or CC-BY-SA (first 5)\")\n",
    "    print(\"=\"*60)\n",
    "    for track in passed_cc_filter[:5]:\n",
    "        print(f\"ID: {track['id']}\")\n",
    "        print(f\"  Name: {track['name']}\")\n",
    "        print(f\"  content_id_free: {track['content_id_free']}\")\n",
    "        print(f\"  License: {track['canonical'].upper()}\")\n",
    "        print(f\"  URL: {track['license_url']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"âŒ REJECTED (first 5)\")\n",
    "    print(\"=\"*60)\n",
    "    for track in rejected_examples[:5]:\n",
    "        print(f\"ID: {track['id']}\")\n",
    "        print(f\"  Name: {track['name']}\")\n",
    "        print(f\"  content_id_free: {track['content_id_free']}\")\n",
    "        print(f\"  License URL: {track['license_url']}\")\n",
    "        print(f\"  Reason: {track['reason']}\")\n",
    "        print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nâš ï¸  INTERRUPTED - Cleaning up resources...\")\n",
    "    # Save checkpoint on interruption\n",
    "    if 'checkpoint' in locals() and 'offset' in locals():\n",
    "        try:\n",
    "            checkpoint['last_offset'] = offset\n",
    "            checkpoint['tracks_fetched'] = len(all_tracks) if 'all_tracks' in locals() else 0\n",
    "            save_checkpoint(checkpoint)\n",
    "            print(f\"âœ“ Checkpoint saved at offset {offset:,}\")\n",
    "            print(f\"âœ“ You can resume by re-running this cell\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to save checkpoint: {e}\")\n",
    "    \n",
    "    # Close checkpoint file if open\n",
    "    if checkpoint_file is not None:\n",
    "        try:\n",
    "            checkpoint_file.flush()\n",
    "            checkpoint_file.close()\n",
    "            print(\"âœ“ Checkpoint file closed\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Close HTTP client\n",
    "    try:\n",
    "        client.close()\n",
    "        print(\"âœ“ HTTP client closed\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"\\nðŸ›‘ Fetch interrupted. Progress has been saved.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Error: {e}\")\n",
    "    # Save checkpoint on error\n",
    "    if 'checkpoint' in locals() and 'offset' in locals():\n",
    "        try:\n",
    "            checkpoint['last_offset'] = offset\n",
    "            checkpoint['tracks_fetched'] = len(all_tracks) if 'all_tracks' in locals() else 0\n",
    "            save_checkpoint(checkpoint)\n",
    "            print(f\"âœ“ Checkpoint saved at offset {offset:,}\")\n",
    "        except:\n",
    "            pass\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    # Ensure resources are always cleaned up\n",
    "    if checkpoint_file is not None:\n",
    "        try:\n",
    "            checkpoint_file.close()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        client.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Download a single audio file and extract comprehensive metadata\n",
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import subprocess\n",
    "import hashlib\n",
    "\n",
    "# Configuration\n",
    "JAMENDO_CLIENT_ID = \"48ecf016\"\n",
    "JAMENDO_API_BASE = \"https://api.jamendo.com/v3.0\"\n",
    "TEST_DOWNLOAD_DIR = Path(\"/Users/cliftonwest/Documents/GitHub/Training/notebooks/jamendo/test_downloads\")\n",
    "TEST_DOWNLOAD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load tracks from checkpoint or final file\n",
    "CHECKPOINT_FILE = Path(\"/Users/cliftonwest/Documents/GitHub/Training/notebooks/jamendo/tracks_checkpoint.jsonl\")\n",
    "OUTPUT_FILE = Path(\"/Users/cliftonwest/Documents/GitHub/Training/notebooks/jamendo/full_track_info.json\")\n",
    "\n",
    "def extract_audio_metadata_ffprobe(filepath):\n",
    "    \"\"\"\n",
    "    Extract technical audio metadata using ffprobe.\n",
    "    Returns dict with: sample_rate_hz, channels, bitrate, codec_name, duration_sec_actual\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run ffprobe to get JSON output\n",
    "        result = subprocess.run([\n",
    "            'ffprobe',\n",
    "            '-v', 'quiet',\n",
    "            '-print_format', 'json',\n",
    "            '-show_format',\n",
    "            '-show_streams',\n",
    "            str(filepath)\n",
    "        ], capture_output=True, text=True, timeout=30)\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            return {\n",
    "                'sample_rate_hz': None,\n",
    "                'channels': None,\n",
    "                'bitrate': None,\n",
    "                'codec_name': None,\n",
    "                'duration_sec_actual': None,\n",
    "                'error': 'ffprobe failed'\n",
    "            }\n",
    "        \n",
    "        data = json.loads(result.stdout)\n",
    "        \n",
    "        # Find audio stream\n",
    "        audio_stream = None\n",
    "        for stream in data.get('streams', []):\n",
    "            if stream.get('codec_type') == 'audio':\n",
    "                audio_stream = stream\n",
    "                break\n",
    "        \n",
    "        if not audio_stream:\n",
    "            return {\n",
    "                'sample_rate_hz': None,\n",
    "                'channels': None,\n",
    "                'bitrate': None,\n",
    "                'codec_name': None,\n",
    "                'duration_sec_actual': None,\n",
    "                'error': 'no audio stream found'\n",
    "            }\n",
    "        \n",
    "        # Extract format info\n",
    "        format_info = data.get('format', {})\n",
    "        \n",
    "        return {\n",
    "            'sample_rate_hz': int(audio_stream.get('sample_rate', 0)) if audio_stream.get('sample_rate') else None,\n",
    "            'channels': audio_stream.get('channels'),\n",
    "            'bitrate': int(format_info.get('bit_rate', 0)) if format_info.get('bit_rate') else None,\n",
    "            'codec_name': audio_stream.get('codec_name'),\n",
    "            'duration_sec_actual': float(format_info.get('duration', 0)) if format_info.get('duration') else None,\n",
    "        }\n",
    "    \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return {\n",
    "            'sample_rate_hz': None,\n",
    "            'channels': None,\n",
    "            'bitrate': None,\n",
    "            'codec_name': None,\n",
    "            'duration_sec_actual': None,\n",
    "            'error': 'ffprobe timeout'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'sample_rate_hz': None,\n",
    "            'channels': None,\n",
    "            'bitrate': None,\n",
    "            'codec_name': None,\n",
    "            'duration_sec_actual': None,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def analyze_audio_quality(filepath):\n",
    "    \"\"\"\n",
    "    Analyze audio quality metrics using ffmpeg.\n",
    "    Returns dict with: peak_dbfs, silence_ratio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use ffmpeg volumedetect filter to get peak volume\n",
    "        result = subprocess.run([\n",
    "            'ffmpeg',\n",
    "            '-i', str(filepath),\n",
    "            '-af', 'volumedetect',\n",
    "            '-f', 'null',\n",
    "            '-'\n",
    "        ], capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        # Parse output for peak volume\n",
    "        peak_dbfs = None\n",
    "        for line in result.stderr.split('\\n'):\n",
    "            if 'max_volume:' in line:\n",
    "                try:\n",
    "                    # Extract value like \"max_volume: -23.5 dB\"\n",
    "                    peak_dbfs = float(line.split(':')[1].strip().split()[0])\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Use ffmpeg silencedetect filter to detect silence\n",
    "        result_silence = subprocess.run([\n",
    "            'ffmpeg',\n",
    "            '-i', str(filepath),\n",
    "            '-af', 'silencedetect=noise=-50dB:d=0.1',\n",
    "            '-f', 'null',\n",
    "            '-'\n",
    "        ], capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        # Parse silence detection output\n",
    "        silence_duration = 0.0\n",
    "        total_duration = 0.0\n",
    "        \n",
    "        for line in result_silence.stderr.split('\\n'):\n",
    "            if 'silence_duration:' in line:\n",
    "                try:\n",
    "                    duration = float(line.split('silence_duration:')[1].strip().split()[0])\n",
    "                    silence_duration += duration\n",
    "                except:\n",
    "                    pass\n",
    "            if 'Duration:' in line and total_duration == 0:\n",
    "                try:\n",
    "                    # Extract duration from \"Duration: 00:03:45.67\"\n",
    "                    time_str = line.split('Duration:')[1].strip().split(',')[0].strip()\n",
    "                    parts = time_str.split(':')\n",
    "                    if len(parts) == 3:\n",
    "                        hours, minutes, seconds = parts\n",
    "                        total_duration = float(hours) * 3600 + float(minutes) * 60 + float(seconds)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        silence_ratio = (silence_duration / total_duration) if total_duration > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'peak_dbfs': peak_dbfs,\n",
    "            'silence_ratio': silence_ratio\n",
    "        }\n",
    "    \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return {\n",
    "            'peak_dbfs': None,\n",
    "            'silence_ratio': None,\n",
    "            'error': 'quality analysis timeout'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'peak_dbfs': None,\n",
    "            'silence_ratio': None,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def compute_file_hash(filepath):\n",
    "    \"\"\"\n",
    "    Compute SHA256 hash of file for deduplication.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sha256_hash = hashlib.sha256()\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            # Read in chunks to handle large files\n",
    "            for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "                sha256_hash.update(chunk)\n",
    "        return sha256_hash.hexdigest()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "print(\"Loading track data...\")\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    # Load first track from checkpoint\n",
    "    with open(CHECKPOINT_FILE, 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        if first_line.strip():\n",
    "            test_track = json.loads(first_line)\n",
    "            print(f\"âœ“ Loaded track from checkpoint file\")\n",
    "        else:\n",
    "            print(\"âœ— Checkpoint file is empty\")\n",
    "            test_track = None\n",
    "elif OUTPUT_FILE.exists():\n",
    "    # Load first track from final file\n",
    "    with open(OUTPUT_FILE, 'r') as f:\n",
    "        all_tracks = json.load(f)\n",
    "        if all_tracks:\n",
    "            test_track = all_tracks[0]\n",
    "            print(f\"âœ“ Loaded track from final output file\")\n",
    "        else:\n",
    "            print(\"âœ— Output file has no tracks\")\n",
    "            test_track = None\n",
    "else:\n",
    "    print(\"âœ— No track data found. Run the fetch cell first.\")\n",
    "    test_track = None\n",
    "\n",
    "if test_track:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEST TRACK INFO\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Track ID: {test_track.get('id')}\")\n",
    "    print(f\"Name: {test_track.get('name')}\")\n",
    "    print(f\"Artist: {test_track.get('artist_name')}\")\n",
    "    print(f\"Album: {test_track.get('album_name')}\")\n",
    "    print(f\"Duration: {test_track.get('duration')}s ({test_track.get('duration')/60:.1f} minutes)\")\n",
    "    \n",
    "    # Get download URL from API\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FETCHING DOWNLOAD URL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    track_id = test_track.get('id')\n",
    "    \n",
    "    client = httpx.Client(timeout=30, follow_redirects=True)\n",
    "    try:\n",
    "        # Request track info with audiodownload format\n",
    "        response = client.get(f\"{JAMENDO_API_BASE}/tracks/\", params={\n",
    "            \"client_id\": JAMENDO_CLIENT_ID,\n",
    "            \"format\": \"json\",\n",
    "            \"id\": track_id,\n",
    "            \"audiodownload\": \"true\"\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        track_data = response.json()\n",
    "        \n",
    "        if track_data.get(\"results\"):\n",
    "            track_info = track_data[\"results\"][0]\n",
    "            download_url = track_info.get(\"audiodownload\")\n",
    "            \n",
    "            if download_url:\n",
    "                print(f\"âœ“ Download URL obtained: {download_url}\")\n",
    "                \n",
    "                # Download the file\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"DOWNLOADING AUDIO FILE\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                # Generate filename\n",
    "                safe_name = \"\".join(c for c in test_track.get('name', 'track') if c.isalnum() or c in (' ', '-', '_')).strip()\n",
    "                safe_artist = \"\".join(c for c in test_track.get('artist_name', 'artist') if c.isalnum() or c in (' ', '-', '_')).strip()\n",
    "                filename = f\"{track_id}_{safe_artist}_{safe_name}.mp3\"\n",
    "                filepath = TEST_DOWNLOAD_DIR / filename\n",
    "                \n",
    "                print(f\"Downloading to: {filepath}\")\n",
    "                print(f\"Starting download...\")\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Stream download with progress\n",
    "                with client.stream(\"GET\", download_url) as r:\n",
    "                    r.raise_for_status()\n",
    "                    total_size = int(r.headers.get('content-length', 0))\n",
    "                    \n",
    "                    with open(filepath, 'wb') as f:\n",
    "                        downloaded = 0\n",
    "                        for chunk in r.iter_bytes(chunk_size=8192):\n",
    "                            f.write(chunk)\n",
    "                            downloaded += len(chunk)\n",
    "                            if total_size > 0:\n",
    "                                progress = (downloaded / total_size) * 100\n",
    "                                print(f\"\\rProgress: {downloaded:,} / {total_size:,} bytes ({progress:.1f}%)\", end='')\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                file_size_mb = filepath.stat().st_size / 1024 / 1024\n",
    "                \n",
    "                print(f\"\\n\\nâœ“ Download complete!\")\n",
    "                print(f\"  File: {filepath.name}\")\n",
    "                print(f\"  Size: {file_size_mb:.2f} MB\")\n",
    "                print(f\"  Time: {elapsed:.1f} seconds\")\n",
    "                print(f\"  Speed: {file_size_mb/elapsed:.2f} MB/s\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # POST-DOWNLOAD METADATA EXTRACTION\n",
    "                # ============================================================\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"EXTRACTING AUDIO METADATA\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                # A) Technical metadata from ffprobe\n",
    "                print(\"Running ffprobe analysis...\")\n",
    "                tech_metadata = extract_audio_metadata_ffprobe(filepath)\n",
    "                \n",
    "                # B) Quality checks from ffmpeg\n",
    "                print(\"Running quality analysis...\")\n",
    "                quality_metadata = analyze_audio_quality(filepath)\n",
    "                \n",
    "                # C) SHA256 hash for deduplication\n",
    "                print(\"Computing file hash...\")\n",
    "                file_hash = compute_file_hash(filepath)\n",
    "                \n",
    "                # D) Segment metadata (not implemented, so None)\n",
    "                segment_metadata = {\n",
    "                    'segment_id': None,\n",
    "                    'segment_start_sec': None,\n",
    "                    'segment_end_sec': None\n",
    "                }\n",
    "                \n",
    "                # Compile complete metadata\n",
    "                complete_metadata = {\n",
    "                    # Original track info\n",
    "                    'track_id': track_id,\n",
    "                    'track_name': test_track.get('name'),\n",
    "                    'artist_name': test_track.get('artist_name'),\n",
    "                    'filename': filename,\n",
    "                    'file_path': str(filepath),\n",
    "                    'file_size_bytes': filepath.stat().st_size,\n",
    "                    \n",
    "                    # A) Technical metadata\n",
    "                    **tech_metadata,\n",
    "                    \n",
    "                    # B) Quality metrics\n",
    "                    **quality_metadata,\n",
    "                    \n",
    "                    # C) Deduplication\n",
    "                    'sha256': file_hash,\n",
    "                    \n",
    "                    # D) Segment metadata\n",
    "                    **segment_metadata\n",
    "                }\n",
    "                \n",
    "                # Print results\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"COMPLETE METADATA\")\n",
    "                print(\"=\"*60)\n",
    "                print(json.dumps(complete_metadata, indent=2))\n",
    "                \n",
    "                # Quality checks summary\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"QUALITY CHECKS SUMMARY\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                # Check for clipping (peak near 0 dBFS)\n",
    "                clipping_detected = False\n",
    "                if complete_metadata['peak_dbfs'] is not None:\n",
    "                    if complete_metadata['peak_dbfs'] > -1.0:\n",
    "                        clipping_detected = True\n",
    "                        print(f\"âš ï¸  CLIPPING DETECTED: Peak = {complete_metadata['peak_dbfs']:.2f} dBFS\")\n",
    "                    else:\n",
    "                        print(f\"âœ“ No clipping: Peak = {complete_metadata['peak_dbfs']:.2f} dBFS\")\n",
    "                else:\n",
    "                    print(\"âš ï¸  Could not determine peak level\")\n",
    "                \n",
    "                # Check for high silence ratio\n",
    "                high_silence = False\n",
    "                if complete_metadata['silence_ratio'] is not None:\n",
    "                    if complete_metadata['silence_ratio'] > 0.20:\n",
    "                        high_silence = True\n",
    "                        print(f\"âš ï¸  HIGH SILENCE RATIO: {complete_metadata['silence_ratio']:.1%} of audio is silent\")\n",
    "                    else:\n",
    "                        print(f\"âœ“ Normal silence ratio: {complete_metadata['silence_ratio']:.1%}\")\n",
    "                else:\n",
    "                    print(\"âš ï¸  Could not determine silence ratio\")\n",
    "                \n",
    "                # Summary counts (for single file)\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"SUMMARY COUNTS\")\n",
    "                print(\"=\"*60)\n",
    "                print(f\"Files analyzed: 1\")\n",
    "                print(f\"Files with clipping: {1 if clipping_detected else 0}\")\n",
    "                print(f\"Files with high silence (>20%): {1 if high_silence else 0}\")\n",
    "                \n",
    "                print(f\"\\nâœ“ Test download and metadata extraction successful!\")\n",
    "                \n",
    "            else:\n",
    "                print(\"âœ— No download URL found in track data\")\n",
    "                print(f\"Available fields: {list(track_info.keys())}\")\n",
    "        else:\n",
    "            print(\"âœ— No track data returned from API\")\n",
    "            \n",
    "    except httpx.HTTPError as e:\n",
    "        print(f\"âœ— HTTP error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        client.close()\n",
    "else:\n",
    "    print(\"\\nSkipping test - no track data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q httpx pandas pyarrow tqdm tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type\n",
    ")\n",
    "from typing import Optional, Dict, List, Any\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_license(license_ccurl: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Parse Creative Commons license URL and return canonical form.\n",
    "    \n",
    "    Returns: 'cc-by', 'cc-by-sa', or None (for rejected licenses)\n",
    "    \n",
    "    Accepts:\n",
    "    - CC-BY 3.0/4.0: http://creativecommons.org/licenses/by/3.0/\n",
    "    - CC-BY-SA 3.0/4.0: http://creativecommons.org/licenses/by-sa/3.0/\n",
    "    \n",
    "    Rejects (returns None):\n",
    "    - CC-BY-NC (NonCommercial)\n",
    "    - CC-BY-ND (NoDerivatives)\n",
    "    - CC-BY-NC-SA\n",
    "    - CC-BY-NC-ND\n",
    "    - Any other license\n",
    "    \"\"\"\n",
    "    if not license_ccurl or not isinstance(license_ccurl, str):\n",
    "        return None\n",
    "    \n",
    "    # Normalize URL\n",
    "    url_lower = license_ccurl.lower().strip().rstrip('/')\n",
    "    \n",
    "    # Extract license type from URL pattern\n",
    "    # Expected: http(s)://creativecommons.org/licenses/{type}/{version}/\n",
    "    if 'creativecommons.org/licenses/' not in url_lower:\n",
    "        return None\n",
    "    \n",
    "    # Extract type component\n",
    "    parts = url_lower.split('creativecommons.org/licenses/')\n",
    "    if len(parts) != 2:\n",
    "        return None\n",
    "    \n",
    "    license_part = parts[1].split('/')[0]  # Get type before version\n",
    "    \n",
    "    # Strict allowlist matching\n",
    "    if license_part == 'by':\n",
    "        return 'cc-by'\n",
    "    elif license_part == 'by-sa':\n",
    "        return 'cc-by-sa'\n",
    "    else:\n",
    "        # Reject: by-nc, by-nd, by-nc-sa, by-nc-nd, etc.\n",
    "        return None\n",
    "\n",
    "# Unit tests\n",
    "assert canonicalize_license(\"http://creativecommons.org/licenses/by/3.0/\") == \"cc-by\"\n",
    "assert canonicalize_license(\"http://creativecommons.org/licenses/by-sa/4.0/\") == \"cc-by-sa\"\n",
    "assert canonicalize_license(\"http://creativecommons.org/licenses/by-nc/3.0/\") is None\n",
    "assert canonicalize_license(\"http://creativecommons.org/licenses/by-nc-sa/3.0/\") is None\n",
    "assert canonicalize_license(\"http://creativecommons.org/licenses/by-nd/3.0/\") is None\n",
    "assert canonicalize_license(None) is None\n",
    "assert canonicalize_license(\"\") is None\n",
    "print(\"âœ“ License canonicalization tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JamendoAPIClient:\n",
    "    \"\"\"Jamendo API client with retry logic and rate limiting.\"\"\"\n",
    "    \n",
    "    def __init__(self, client_id: str, timeout: int = 30):\n",
    "        self.client_id = client_id\n",
    "        self.timeout = timeout\n",
    "        self.client = httpx.Client(timeout=timeout)\n",
    "        self.last_request_time = 0\n",
    "        self.min_request_interval = 0.1  # 100ms between requests\n",
    "    \n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Simple rate limiting: ensure minimum interval between requests.\"\"\"\n",
    "        elapsed = time.time() - self.last_request_time\n",
    "        if elapsed < self.min_request_interval:\n",
    "            time.sleep(self.min_request_interval - elapsed)\n",
    "        self.last_request_time = time.time()\n",
    "    \n",
    "    @retry(\n",
    "        stop=stop_after_attempt(RETRY_MAX_ATTEMPTS),\n",
    "        wait=wait_exponential(multiplier=RETRY_BACKOFF_FACTOR, min=1, max=60),\n",
    "        retry=retry_if_exception_type((httpx.TimeoutException, httpx.HTTPStatusError))\n",
    "    )\n",
    "    def fetch_tracks(\n",
    "        self, \n",
    "        offset: int = 0, \n",
    "        limit: int = 200,\n",
    "        include_fullcount: bool = False\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch tracks from Jamendo API with pagination.\n",
    "        \n",
    "        Returns: {\"headers\": {...}, \"results\": [...]}\n",
    "        \"\"\"\n",
    "        self._rate_limit()\n",
    "        \n",
    "        params = {\n",
    "            \"client_id\": self.client_id,\n",
    "            \"format\": \"json\",\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset,\n",
    "            \"audiodownload\": \"true\",  # Only downloadable tracks\n",
    "        }\n",
    "        \n",
    "        if include_fullcount:\n",
    "            params[\"fullcount\"] = \"true\"\n",
    "        \n",
    "        url = f\"{JAMENDO_API_BASE}/tracks/\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            if e.response.status_code == 429:\n",
    "                # Rate limit - tenacity will retry with backoff\n",
    "                print(f\"âš  Rate limited (429), will retry...\")\n",
    "                raise\n",
    "            elif e.response.status_code >= 500:\n",
    "                # Server error - tenacity will retry\n",
    "                print(f\"âš  Server error ({e.response.status_code}), will retry...\")\n",
    "                raise\n",
    "            else:\n",
    "                # Client error - don't retry\n",
    "                print(f\"âœ— Client error ({e.response.status_code}): {e}\")\n",
    "                raise RuntimeError(f\"API error: {e.response.status_code}\") from e\n",
    "    \n",
    "    def close(self):\n",
    "        self.client.close()\n",
    "\n",
    "# Test client initialization\n",
    "api_client = JamendoAPIClient(JAMENDO_CLIENT_ID, timeout=REQUEST_TIMEOUT)\n",
    "print(\"âœ“ API client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state() -> Dict[str, Any]:\n",
    "    \"\"\"Load checkpoint state or return initial state.\"\"\"\n",
    "    if STATE_FILE.exists():\n",
    "        with open(STATE_FILE, 'r') as f:\n",
    "            state = json.load(f)\n",
    "        print(f\"âœ“ Loaded checkpoint: offset={state['last_offset']}, fetched={state['total_fetched']}\")\n",
    "        return state\n",
    "    else:\n",
    "        return {\n",
    "            \"last_offset\": 0,\n",
    "            \"total_fetched\": 0,\n",
    "            \"total_passed_filter\": 0,\n",
    "            \"total_rejected\": 0,\n",
    "            \"total_catalog_size\": None,\n",
    "            \"start_time\": datetime.utcnow().isoformat(),\n",
    "            \"last_update_time\": None\n",
    "        }\n",
    "\n",
    "def save_state(state: Dict[str, Any]):\n",
    "    \"\"\"Save checkpoint state.\"\"\"\n",
    "    state[\"last_update_time\"] = datetime.utcnow().isoformat()\n",
    "    with open(STATE_FILE, 'w') as f:\n",
    "        json.dump(state, f, indent=2)\n",
    "\n",
    "# Load or initialize state\n",
    "state = load_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_track(track: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Filter and extract metadata from a track.\n",
    "    Returns None if track doesn't pass license filter.\n",
    "    \"\"\"\n",
    "    license_url = track.get(\"license_ccurl\")\n",
    "    canonical_license = canonicalize_license(license_url)\n",
    "    \n",
    "    if canonical_license is None:\n",
    "        return None\n",
    "    \n",
    "    # Extract relevant metadata\n",
    "    return {\n",
    "        \"id\": track.get(\"id\"),\n",
    "        \"name\": track.get(\"name\"),\n",
    "        \"duration\": track.get(\"duration\"),  # in seconds\n",
    "        \"artist_id\": track.get(\"artist_id\"),\n",
    "        \"artist_name\": track.get(\"artist_name\"),\n",
    "        \"album_id\": track.get(\"album_id\"),\n",
    "        \"album_name\": track.get(\"album_name\"),\n",
    "        \"license\": canonical_license,\n",
    "        \"license_ccurl\": license_url,\n",
    "        \"releasedate\": track.get(\"releasedate\"),\n",
    "        \"audiodownload_allowed\": track.get(\"audiodownload_allowed\"),\n",
    "    }\n",
    "\n",
    "# Main crawl loop\n",
    "offset = state[\"last_offset\"]\n",
    "page_count = 0\n",
    "audit_passed = []\n",
    "audit_rejected = []\n",
    "\n",
    "# Open JSONL in append mode\n",
    "jsonl_mode = 'a' if offset > 0 else 'w'\n",
    "jsonl_file = open(JSONL_FILE, jsonl_mode)\n",
    "\n",
    "try:\n",
    "    # Get total catalog size on first request\n",
    "    if state[\"total_catalog_size\"] is None:\n",
    "        print(\"Fetching catalog size...\")\n",
    "        first_response = api_client.fetch_tracks(offset=0, limit=1, include_fullcount=True)\n",
    "        total_size = first_response[\"headers\"].get(\"results_fullcount\", 0)\n",
    "        state[\"total_catalog_size\"] = total_size\n",
    "        print(f\"Total catalog size: {total_size:,} tracks\")\n",
    "    \n",
    "    total_size = state[\"total_catalog_size\"]\n",
    "    total_pages = (total_size // PAGE_SIZE) + (1 if total_size % PAGE_SIZE else 0)\n",
    "    \n",
    "    if MAX_PAGES > 0:\n",
    "        total_pages = min(total_pages, MAX_PAGES)\n",
    "        print(f\"DRY RUN: Processing {total_pages} pages only\")\n",
    "    \n",
    "    # Progress bar\n",
    "    with tqdm(total=total_pages, initial=offset // PAGE_SIZE, desc=\"Crawling pages\", unit=\"page\") as pbar:\n",
    "        while True:\n",
    "            # Check if we've reached max pages (dry run mode)\n",
    "            if MAX_PAGES > 0 and page_count >= MAX_PAGES:\n",
    "                print(f\"Reached MAX_PAGES limit ({MAX_PAGES})\")\n",
    "                break\n",
    "            \n",
    "            # Check if we've exhausted the catalog\n",
    "            if offset >= total_size:\n",
    "                print(f\"Completed: processed all {total_size:,} tracks\")\n",
    "                break\n",
    "            \n",
    "            # Fetch page\n",
    "            response = api_client.fetch_tracks(offset=offset, limit=PAGE_SIZE)\n",
    "            results = response.get(\"results\", [])\n",
    "            \n",
    "            if not results:\n",
    "                print(f\"No more results at offset {offset}\")\n",
    "                break\n",
    "            \n",
    "            # Process tracks\n",
    "            for track in results:\n",
    "                state[\"total_fetched\"] += 1\n",
    "                \n",
    "                processed = process_track(track)\n",
    "                if processed:\n",
    "                    # Write to JSONL immediately (streaming)\n",
    "                    jsonl_file.write(json.dumps(processed) + '\\n')\n",
    "                    state[\"total_passed_filter\"] += 1\n",
    "                    \n",
    "                    # Collect audit samples\n",
    "                    if len(audit_passed) < 10:\n",
    "                        audit_passed.append({\n",
    "                            \"id\": track.get(\"id\"),\n",
    "                            \"name\": track.get(\"name\"),\n",
    "                            \"license_ccurl\": track.get(\"license_ccurl\"),\n",
    "                            \"canonical\": processed[\"license\"]\n",
    "                        })\n",
    "                else:\n",
    "                    state[\"total_rejected\"] += 1\n",
    "                    \n",
    "                    # Collect audit samples\n",
    "                    if len(audit_rejected) < 10:\n",
    "                        audit_rejected.append({\n",
    "                            \"id\": track.get(\"id\"),\n",
    "                            \"name\": track.get(\"name\"),\n",
    "                            \"license_ccurl\": track.get(\"license_ccurl\"),\n",
    "                            \"reason\": \"license_not_allowed\"\n",
    "                        })\n",
    "            \n",
    "            # Update state\n",
    "            offset += len(results)\n",
    "            state[\"last_offset\"] = offset\n",
    "            page_count += 1\n",
    "            \n",
    "            # Checkpoint periodically\n",
    "            if page_count % CHECKPOINT_INTERVAL == 0:\n",
    "                jsonl_file.flush()\n",
    "                save_state(state)\n",
    "            \n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\n",
    "                \"passed\": state[\"total_passed_filter\"],\n",
    "                \"rejected\": state[\"total_rejected\"]\n",
    "            })\n",
    "    \n",
    "    # Final checkpoint\n",
    "    jsonl_file.flush()\n",
    "    save_state(state)\n",
    "    \n",
    "finally:\n",
    "    jsonl_file.close()\n",
    "    api_client.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CRAWL COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total tracks fetched: {state['total_fetched']:,}\")\n",
    "print(f\"Passed filter (CC-BY/CC-BY-SA): {state['total_passed_filter']:,}\")\n",
    "print(f\"Rejected (NC/ND/other): {state['total_rejected']:,}\")\n",
    "print(f\"Pass rate: {100 * state['total_passed_filter'] / state['total_fetched']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AUDIT TRAIL - Sample Tracks\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nâœ“ PASSED FILTER (CC-BY/CC-BY-SA):\")\n",
    "for i, track in enumerate(audit_passed, 1):\n",
    "    print(f\"{i}. [{track['id']}] {track['name']}\")\n",
    "    print(f\"   License URL: {track['license_ccurl']}\")\n",
    "    print(f\"   Canonical: {track['canonical']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nâœ— REJECTED (NC/ND/other):\")\n",
    "for i, track in enumerate(audit_rejected, 1):\n",
    "    print(f\"{i}. [{track['id']}] {track['name']}\")\n",
    "    print(f\"   License URL: {track['license_ccurl']}\")\n",
    "    print(f\"   Reason: {track['reason']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSONL into DataFrame\n",
    "print(\"Loading metadata for aggregation...\")\n",
    "df = pd.read_json(JSONL_FILE, lines=True)\n",
    "print(f\"Loaded {len(df):,} tracks\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "total_tracks = len(df)\n",
    "total_duration_seconds = df['duration'].sum()\n",
    "total_hours = total_duration_seconds / 3600\n",
    "\n",
    "summary = {\n",
    "    \"generated_at\": datetime.utcnow().isoformat(),\n",
    "    \"source\": \"jamendo_api\",\n",
    "    \"license_filter\": list(ALLOWED_LICENSES),\n",
    "    \"total_tracks\": int(total_tracks),\n",
    "    \"total_duration_seconds\": float(total_duration_seconds),\n",
    "    \"total_duration_hours\": float(total_hours),\n",
    "    \"total_catalog_tracks_fetched\": state[\"total_fetched\"],\n",
    "    \"pass_rate_percent\": float(100 * state[\"total_passed_filter\"] / state['total_fetched']),\n",
    "    \"crawl_start_time\": state[\"start_time\"],\n",
    "    \"crawl_end_time\": state[\"last_update_time\"],\n",
    "}\n",
    "\n",
    "# Save summary JSON\n",
    "with open(SUMMARY_FILE, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total tracks: {summary['total_tracks']:,}\")\n",
    "print(f\"Total duration: {summary['total_duration_hours']:,.1f} hours\")\n",
    "print(f\"Average track duration: {total_duration_seconds / total_tracks / 60:.1f} minutes\")\n",
    "print(f\"Pass rate: {summary['pass_rate_percent']:.1f}%\")\n",
    "print(f\"\\nâœ“ Saved: {SUMMARY_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by license type\n",
    "license_stats = df.groupby('license').agg({\n",
    "    'duration': ['count', 'sum']\n",
    "}).reset_index()\n",
    "\n",
    "license_stats.columns = ['license', 'track_count', 'total_duration_seconds']\n",
    "license_stats['total_duration_hours'] = license_stats['total_duration_seconds'] / 3600\n",
    "license_stats['percentage_of_tracks'] = 100 * license_stats['track_count'] / total_tracks\n",
    "\n",
    "# Save CSV\n",
    "license_stats.to_csv(LICENSE_CSV, index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BREAKDOWN BY LICENSE TYPE\")\n",
    "print(\"=\"*60)\n",
    "print(license_stats.to_string(index=False))\n",
    "print(f\"\\nâœ“ Saved: {LICENSE_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute duration percentiles\n",
    "duration_minutes = df['duration'] / 60\n",
    "\n",
    "duration_stats = pd.DataFrame({\n",
    "    'metric': ['mean', 'median (p50)', 'p90', 'p95', 'p99', 'min', 'max'],\n",
    "    'duration_minutes': [\n",
    "        duration_minutes.mean(),\n",
    "        duration_minutes.quantile(0.50),\n",
    "        duration_minutes.quantile(0.90),\n",
    "        duration_minutes.quantile(0.95),\n",
    "        duration_minutes.quantile(0.99),\n",
    "        duration_minutes.min(),\n",
    "        duration_minutes.max(),\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Save CSV\n",
    "duration_stats.to_csv(DURATION_CSV, index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DURATION STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(duration_stats.to_string(index=False))\n",
    "print(f\"\\nâœ“ Saved: {DURATION_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Parquet (more efficient for large datasets)\n",
    "try:\n",
    "    df.to_parquet(PARQUET_FILE, index=False, compression='snappy')\n",
    "    print(f\"âœ“ Saved Parquet: {PARQUET_FILE}\")\n",
    "    print(f\"  JSONL size: {JSONL_FILE.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "    print(f\"  Parquet size: {PARQUET_FILE.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Failed to save Parquet: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL OUTPUTS GENERATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  1. {JSONL_FILE.name} - Raw filtered metadata (append-only)\")\n",
    "print(f\"  2. {SUMMARY_FILE.name} - Topline metrics\")\n",
    "print(f\"  3. {LICENSE_CSV.name} - License breakdown\")\n",
    "print(f\"  4. {DURATION_CSV.name} - Duration statistics\")\n",
    "print(f\"  5. {STATE_FILE.name} - Checkpoint state (for resume)\")\n",
    "if PARQUET_FILE.exists():\n",
    "    print(f\"  6. {PARQUET_FILE.name} - Parquet archive (optional)\")\n",
    "\n",
    "print(f\"\\nâœ“ Pipeline complete!\")\n",
    "print(f\"\\nTo resume crawl if interrupted:\")\n",
    "print(f\"  - Re-run this notebook (it will resume from offset {state['last_offset']})\")\n",
    "print(f\"\\nTo start fresh:\")\n",
    "print(f\"  - Delete {STATE_FILE} and {JSONL_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
